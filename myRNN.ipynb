{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "01f79c48-e625-4c02-9bc4-2ea791952323",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cc6939cb-4aca-4424-aa6e-2eb4b8d6ad1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.1\n"
     ]
    }
   ],
   "source": [
    "print(torch.__version__)\n",
    "\n",
    "# Setup computational device based on CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b4f00de-7b43-427e-9ecc-f7ab6af00f80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire and organize MNIST training dataset\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=1)\n",
    "\n",
    "# Acquire and organize MNIST testing dataset\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "# Group data loaders into a dictionary for ease of access\n",
    "data_loaders = {'train': train_loader, 'test': test_loader}\n",
    "# [1] https://www.kaggle.com/code/fgiorgio/multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "cd1ddf1f-fcda-4c04-a8a6-d4bdb514b053",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "seq_length = 28\n",
    "input_dimensions = 28\n",
    "rnn_layers = 2\n",
    "output_classes = 10\n",
    "batch_sz = 100\n",
    "hidden_unit = 128\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005]\n",
    "num_epochs = [10, 20, 50]\n",
    "\n",
    "param_combinations = list(itertools.product(learning_rates, num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0b9b4316-12bd-450c-ba85-cf5e24b38db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct RNN architecture\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, rnn_layers, output_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_units, rnn_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, output_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states for LSTM layers\n",
    "        h0 = torch.zeros(self.rnn_layers, x.size(0), self.hidden_units).to(device)\n",
    "        c0 = torch.zeros(self.rnn_layers, x.size(0), self.hidden_units).to(device)\n",
    "        \n",
    "        # Feed data through recurrent layers and obtain last output\n",
    "        out, _ = self.rnn(x, (h0, c0))  # Tuple of (hidden state, cell state)\n",
    "        \n",
    "        # Adapt the output for the final classification layer\n",
    "        out = self.fc(out[:, -1, :])  # Get the last time step output for each batch\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23950a40-ffd8-4888-9781-a122f28eb01a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.01, epoch=10\n",
      "Epoch [1/10], Step [100/ 600], Loss: 0.5282\n",
      "Epoch [1/10], Step [200/ 600], Loss: 0.3638\n",
      "Epoch [1/10], Step [300/ 600], Loss: 0.2812\n",
      "Epoch [1/10], Step [400/ 600], Loss: 0.3004\n",
      "Epoch [1/10], Step [500/ 600], Loss: 0.1060\n",
      "Epoch [1/10], Step [600/ 600], Loss: 0.1280\n",
      "Epoch [2/10], Step [100/ 600], Loss: 0.0552\n",
      "Epoch [2/10], Step [200/ 600], Loss: 0.1195\n",
      "Epoch [2/10], Step [300/ 600], Loss: 0.0303\n",
      "Epoch [2/10], Step [400/ 600], Loss: 0.0955\n",
      "Epoch [2/10], Step [500/ 600], Loss: 0.0573\n",
      "Epoch [2/10], Step [600/ 600], Loss: 0.1726\n",
      "Epoch [3/10], Step [100/ 600], Loss: 0.0780\n",
      "Epoch [3/10], Step [200/ 600], Loss: 0.0959\n",
      "Epoch [3/10], Step [300/ 600], Loss: 0.0778\n",
      "Epoch [3/10], Step [400/ 600], Loss: 0.1002\n",
      "Epoch [3/10], Step [500/ 600], Loss: 0.0137\n",
      "Epoch [3/10], Step [600/ 600], Loss: 0.1489\n",
      "Epoch [4/10], Step [100/ 600], Loss: 0.0432\n",
      "Epoch [4/10], Step [200/ 600], Loss: 0.0677\n",
      "Epoch [4/10], Step [300/ 600], Loss: 0.0964\n",
      "Epoch [4/10], Step [400/ 600], Loss: 0.0169\n",
      "Epoch [4/10], Step [500/ 600], Loss: 0.0566\n",
      "Epoch [4/10], Step [600/ 600], Loss: 0.0255\n",
      "Epoch [5/10], Step [100/ 600], Loss: 0.0671\n",
      "Epoch [5/10], Step [200/ 600], Loss: 0.0560\n",
      "Epoch [5/10], Step [300/ 600], Loss: 0.0174\n",
      "Epoch [5/10], Step [400/ 600], Loss: 0.1382\n",
      "Epoch [5/10], Step [500/ 600], Loss: 0.0236\n",
      "Epoch [5/10], Step [600/ 600], Loss: 0.0134\n",
      "Epoch [6/10], Step [100/ 600], Loss: 0.0349\n",
      "Epoch [6/10], Step [200/ 600], Loss: 0.0127\n",
      "Epoch [6/10], Step [300/ 600], Loss: 0.0658\n",
      "Epoch [6/10], Step [400/ 600], Loss: 0.0214\n",
      "Epoch [6/10], Step [500/ 600], Loss: 0.0194\n",
      "Epoch [6/10], Step [600/ 600], Loss: 0.0424\n",
      "Epoch [7/10], Step [100/ 600], Loss: 0.1220\n",
      "Epoch [7/10], Step [200/ 600], Loss: 0.0800\n",
      "Epoch [7/10], Step [300/ 600], Loss: 0.0669\n",
      "Epoch [7/10], Step [400/ 600], Loss: 0.0345\n",
      "Epoch [7/10], Step [500/ 600], Loss: 0.0321\n",
      "Epoch [7/10], Step [600/ 600], Loss: 0.0537\n",
      "Epoch [8/10], Step [100/ 600], Loss: 0.0048\n",
      "Epoch [8/10], Step [200/ 600], Loss: 0.0170\n",
      "Epoch [8/10], Step [300/ 600], Loss: 0.0017\n",
      "Epoch [8/10], Step [400/ 600], Loss: 0.0115\n",
      "Epoch [8/10], Step [500/ 600], Loss: 0.0096\n",
      "Epoch [8/10], Step [600/ 600], Loss: 0.0288\n",
      "Epoch [9/10], Step [100/ 600], Loss: 0.0219\n",
      "Epoch [9/10], Step [200/ 600], Loss: 0.0452\n",
      "Epoch [9/10], Step [300/ 600], Loss: 0.1443\n",
      "Epoch [9/10], Step [400/ 600], Loss: 0.0604\n",
      "Epoch [9/10], Step [500/ 600], Loss: 0.0427\n",
      "Epoch [9/10], Step [600/ 600], Loss: 0.0242\n",
      "Epoch [10/10], Step [100/ 600], Loss: 0.0595\n",
      "Epoch [10/10], Step [200/ 600], Loss: 0.0739\n",
      "Epoch [10/10], Step [300/ 600], Loss: 0.0586\n",
      "Epoch [10/10], Step [400/ 600], Loss: 0.0218\n",
      "Epoch [10/10], Step [500/ 600], Loss: 0.0199\n",
      "Epoch [10/10], Step [600/ 600], Loss: 0.0513\n",
      "Test Accuracy of the model on the 10000 test images:98.41%\n",
      "Total execution time: 304.378653049469 seconds\n",
      "Training with lr=0.01, epoch=20\n",
      "Epoch [1/20], Step [100/ 600], Loss: 0.6981\n",
      "Epoch [1/20], Step [200/ 600], Loss: 0.1543\n",
      "Epoch [1/20], Step [300/ 600], Loss: 0.2387\n",
      "Epoch [1/20], Step [400/ 600], Loss: 0.0949\n",
      "Epoch [1/20], Step [500/ 600], Loss: 0.2159\n",
      "Epoch [1/20], Step [600/ 600], Loss: 0.1238\n",
      "Epoch [2/20], Step [100/ 600], Loss: 0.2219\n",
      "Epoch [2/20], Step [200/ 600], Loss: 0.1421\n",
      "Epoch [2/20], Step [300/ 600], Loss: 0.0448\n",
      "Epoch [2/20], Step [400/ 600], Loss: 0.0709\n",
      "Epoch [2/20], Step [500/ 600], Loss: 0.1034\n",
      "Epoch [2/20], Step [600/ 600], Loss: 0.2265\n",
      "Epoch [3/20], Step [100/ 600], Loss: 0.1709\n",
      "Epoch [3/20], Step [200/ 600], Loss: 0.1342\n",
      "Epoch [3/20], Step [300/ 600], Loss: 0.0941\n",
      "Epoch [3/20], Step [400/ 600], Loss: 0.1422\n",
      "Epoch [3/20], Step [500/ 600], Loss: 0.0686\n",
      "Epoch [3/20], Step [600/ 600], Loss: 0.0376\n",
      "Epoch [4/20], Step [100/ 600], Loss: 0.0195\n",
      "Epoch [4/20], Step [200/ 600], Loss: 0.0778\n",
      "Epoch [4/20], Step [300/ 600], Loss: 0.1172\n",
      "Epoch [4/20], Step [400/ 600], Loss: 0.0200\n",
      "Epoch [4/20], Step [500/ 600], Loss: 0.0420\n",
      "Epoch [4/20], Step [600/ 600], Loss: 0.0495\n",
      "Epoch [5/20], Step [100/ 600], Loss: 0.0342\n",
      "Epoch [5/20], Step [200/ 600], Loss: 0.0400\n",
      "Epoch [5/20], Step [300/ 600], Loss: 0.0117\n",
      "Epoch [5/20], Step [400/ 600], Loss: 0.0921\n",
      "Epoch [5/20], Step [500/ 600], Loss: 0.0359\n",
      "Epoch [5/20], Step [600/ 600], Loss: 0.0193\n",
      "Epoch [6/20], Step [100/ 600], Loss: 0.0788\n",
      "Epoch [6/20], Step [200/ 600], Loss: 0.1207\n",
      "Epoch [6/20], Step [300/ 600], Loss: 0.0518\n",
      "Epoch [6/20], Step [400/ 600], Loss: 0.0875\n",
      "Epoch [6/20], Step [500/ 600], Loss: 0.0418\n",
      "Epoch [6/20], Step [600/ 600], Loss: 0.0357\n",
      "Epoch [7/20], Step [100/ 600], Loss: 0.0286\n",
      "Epoch [7/20], Step [200/ 600], Loss: 0.1325\n",
      "Epoch [7/20], Step [300/ 600], Loss: 0.0135\n",
      "Epoch [7/20], Step [400/ 600], Loss: 0.0728\n",
      "Epoch [7/20], Step [500/ 600], Loss: 0.0233\n",
      "Epoch [7/20], Step [600/ 600], Loss: 0.1365\n",
      "Epoch [8/20], Step [100/ 600], Loss: 0.0420\n",
      "Epoch [8/20], Step [200/ 600], Loss: 0.0146\n",
      "Epoch [8/20], Step [300/ 600], Loss: 0.0456\n",
      "Epoch [8/20], Step [400/ 600], Loss: 0.0114\n",
      "Epoch [8/20], Step [500/ 600], Loss: 0.1244\n",
      "Epoch [8/20], Step [600/ 600], Loss: 0.1460\n",
      "Epoch [9/20], Step [100/ 600], Loss: 0.0535\n",
      "Epoch [9/20], Step [200/ 600], Loss: 0.0688\n",
      "Epoch [9/20], Step [300/ 600], Loss: 0.0410\n",
      "Epoch [9/20], Step [400/ 600], Loss: 0.0101\n",
      "Epoch [9/20], Step [500/ 600], Loss: 0.0665\n",
      "Epoch [9/20], Step [600/ 600], Loss: 0.0765\n",
      "Epoch [10/20], Step [100/ 600], Loss: 0.1050\n",
      "Epoch [10/20], Step [200/ 600], Loss: 0.0402\n",
      "Epoch [10/20], Step [300/ 600], Loss: 0.0308\n",
      "Epoch [10/20], Step [400/ 600], Loss: 0.1766\n",
      "Epoch [10/20], Step [500/ 600], Loss: 0.1290\n",
      "Epoch [10/20], Step [600/ 600], Loss: 0.0209\n",
      "Epoch [11/20], Step [100/ 600], Loss: 0.0387\n",
      "Epoch [11/20], Step [200/ 600], Loss: 0.0595\n",
      "Epoch [11/20], Step [300/ 600], Loss: 0.0201\n",
      "Epoch [11/20], Step [400/ 600], Loss: 0.0265\n",
      "Epoch [11/20], Step [500/ 600], Loss: 0.1008\n",
      "Epoch [11/20], Step [600/ 600], Loss: 0.0720\n",
      "Epoch [12/20], Step [100/ 600], Loss: 0.0372\n",
      "Epoch [12/20], Step [200/ 600], Loss: 0.0808\n",
      "Epoch [12/20], Step [300/ 600], Loss: 0.0752\n",
      "Epoch [12/20], Step [400/ 600], Loss: 0.0654\n",
      "Epoch [12/20], Step [500/ 600], Loss: 0.0697\n",
      "Epoch [12/20], Step [600/ 600], Loss: 0.0724\n",
      "Epoch [13/20], Step [100/ 600], Loss: 0.1159\n",
      "Epoch [13/20], Step [200/ 600], Loss: 0.0877\n",
      "Epoch [13/20], Step [300/ 600], Loss: 0.0289\n",
      "Epoch [13/20], Step [400/ 600], Loss: 0.2192\n",
      "Epoch [13/20], Step [500/ 600], Loss: 0.2117\n",
      "Epoch [13/20], Step [600/ 600], Loss: 0.4010\n",
      "Epoch [14/20], Step [100/ 600], Loss: 0.2066\n",
      "Epoch [14/20], Step [200/ 600], Loss: 0.2038\n",
      "Epoch [14/20], Step [300/ 600], Loss: 0.0922\n",
      "Epoch [14/20], Step [400/ 600], Loss: 0.4783\n",
      "Epoch [14/20], Step [500/ 600], Loss: 0.3822\n",
      "Epoch [14/20], Step [600/ 600], Loss: 0.4675\n",
      "Epoch [15/20], Step [100/ 600], Loss: 0.2358\n",
      "Epoch [15/20], Step [200/ 600], Loss: 0.2088\n",
      "Epoch [15/20], Step [300/ 600], Loss: 0.1877\n",
      "Epoch [15/20], Step [400/ 600], Loss: 0.2640\n",
      "Epoch [15/20], Step [500/ 600], Loss: 0.4944\n",
      "Epoch [15/20], Step [600/ 600], Loss: 0.1696\n",
      "Epoch [16/20], Step [100/ 600], Loss: 0.3940\n",
      "Epoch [16/20], Step [200/ 600], Loss: 0.1927\n",
      "Epoch [16/20], Step [300/ 600], Loss: 0.2739\n",
      "Epoch [16/20], Step [400/ 600], Loss: 0.0928\n",
      "Epoch [16/20], Step [500/ 600], Loss: 0.1211\n",
      "Epoch [16/20], Step [600/ 600], Loss: 0.2022\n",
      "Epoch [17/20], Step [100/ 600], Loss: 0.0947\n",
      "Epoch [17/20], Step [200/ 600], Loss: 0.1762\n",
      "Epoch [17/20], Step [300/ 600], Loss: 0.0743\n",
      "Epoch [17/20], Step [400/ 600], Loss: 0.1531\n",
      "Epoch [17/20], Step [500/ 600], Loss: 0.1572\n",
      "Epoch [17/20], Step [600/ 600], Loss: 0.2790\n",
      "Epoch [18/20], Step [100/ 600], Loss: 0.2222\n",
      "Epoch [18/20], Step [200/ 600], Loss: 0.1372\n",
      "Epoch [18/20], Step [300/ 600], Loss: 0.1682\n",
      "Epoch [18/20], Step [400/ 600], Loss: 0.1183\n",
      "Epoch [18/20], Step [500/ 600], Loss: 0.1896\n",
      "Epoch [18/20], Step [600/ 600], Loss: 0.1497\n",
      "Epoch [19/20], Step [100/ 600], Loss: 0.2297\n",
      "Epoch [19/20], Step [200/ 600], Loss: 0.1499\n",
      "Epoch [19/20], Step [300/ 600], Loss: 0.1182\n",
      "Epoch [19/20], Step [400/ 600], Loss: 0.1747\n",
      "Epoch [19/20], Step [500/ 600], Loss: 0.1910\n",
      "Epoch [19/20], Step [600/ 600], Loss: 0.3901\n",
      "Epoch [20/20], Step [100/ 600], Loss: 0.4984\n",
      "Epoch [20/20], Step [200/ 600], Loss: 0.9317\n",
      "Epoch [20/20], Step [300/ 600], Loss: 0.7342\n",
      "Epoch [20/20], Step [400/ 600], Loss: 0.5801\n",
      "Epoch [20/20], Step [500/ 600], Loss: 0.4452\n",
      "Epoch [20/20], Step [600/ 600], Loss: 0.3445\n",
      "Test Accuracy of the model on the 10000 test images:88.86%\n",
      "Total execution time: 630.4483232498169 seconds\n",
      "Training with lr=0.01, epoch=50\n",
      "Epoch [1/50], Step [100/ 600], Loss: 0.5324\n",
      "Epoch [1/50], Step [200/ 600], Loss: 0.2183\n",
      "Epoch [1/50], Step [300/ 600], Loss: 0.1839\n",
      "Epoch [1/50], Step [400/ 600], Loss: 0.1262\n",
      "Epoch [1/50], Step [500/ 600], Loss: 0.0786\n",
      "Epoch [1/50], Step [600/ 600], Loss: 0.0724\n",
      "Epoch [2/50], Step [100/ 600], Loss: 0.1251\n",
      "Epoch [2/50], Step [200/ 600], Loss: 0.0650\n",
      "Epoch [2/50], Step [300/ 600], Loss: 0.1091\n",
      "Epoch [2/50], Step [400/ 600], Loss: 0.2178\n",
      "Epoch [2/50], Step [500/ 600], Loss: 0.0211\n",
      "Epoch [2/50], Step [600/ 600], Loss: 0.0299\n",
      "Epoch [3/50], Step [100/ 600], Loss: 0.0570\n",
      "Epoch [3/50], Step [200/ 600], Loss: 0.0951\n",
      "Epoch [3/50], Step [300/ 600], Loss: 0.0345\n",
      "Epoch [3/50], Step [400/ 600], Loss: 0.0381\n",
      "Epoch [3/50], Step [500/ 600], Loss: 0.0242\n",
      "Epoch [3/50], Step [600/ 600], Loss: 0.0852\n",
      "Epoch [4/50], Step [100/ 600], Loss: 0.0604\n",
      "Epoch [4/50], Step [200/ 600], Loss: 0.0439\n",
      "Epoch [4/50], Step [300/ 600], Loss: 0.1183\n",
      "Epoch [4/50], Step [400/ 600], Loss: 0.1791\n",
      "Epoch [4/50], Step [500/ 600], Loss: 0.0920\n",
      "Epoch [4/50], Step [600/ 600], Loss: 0.0615\n",
      "Epoch [5/50], Step [100/ 600], Loss: 0.0935\n",
      "Epoch [5/50], Step [200/ 600], Loss: 0.0930\n",
      "Epoch [5/50], Step [300/ 600], Loss: 0.0195\n",
      "Epoch [5/50], Step [400/ 600], Loss: 0.0084\n",
      "Epoch [5/50], Step [500/ 600], Loss: 0.0815\n",
      "Epoch [5/50], Step [600/ 600], Loss: 0.0299\n",
      "Epoch [6/50], Step [100/ 600], Loss: 0.0275\n",
      "Epoch [6/50], Step [200/ 600], Loss: 0.0559\n",
      "Epoch [6/50], Step [300/ 600], Loss: 0.0494\n",
      "Epoch [6/50], Step [400/ 600], Loss: 0.0438\n",
      "Epoch [6/50], Step [500/ 600], Loss: 0.0763\n",
      "Epoch [6/50], Step [600/ 600], Loss: 0.0113\n",
      "Epoch [7/50], Step [100/ 600], Loss: 0.0846\n",
      "Epoch [7/50], Step [200/ 600], Loss: 0.0708\n",
      "Epoch [7/50], Step [300/ 600], Loss: 0.0308\n",
      "Epoch [7/50], Step [400/ 600], Loss: 0.1138\n",
      "Epoch [7/50], Step [500/ 600], Loss: 0.0253\n",
      "Epoch [7/50], Step [600/ 600], Loss: 0.0289\n",
      "Epoch [8/50], Step [100/ 600], Loss: 0.0509\n",
      "Epoch [8/50], Step [200/ 600], Loss: 0.0350\n",
      "Epoch [8/50], Step [300/ 600], Loss: 0.1439\n",
      "Epoch [8/50], Step [400/ 600], Loss: 0.0619\n",
      "Epoch [8/50], Step [500/ 600], Loss: 0.1773\n",
      "Epoch [8/50], Step [600/ 600], Loss: 0.0469\n",
      "Epoch [9/50], Step [100/ 600], Loss: 0.0227\n",
      "Epoch [9/50], Step [200/ 600], Loss: 0.0952\n",
      "Epoch [9/50], Step [300/ 600], Loss: 0.2241\n",
      "Epoch [9/50], Step [400/ 600], Loss: 0.0987\n",
      "Epoch [9/50], Step [500/ 600], Loss: 0.0840\n",
      "Epoch [9/50], Step [600/ 600], Loss: 0.1019\n",
      "Epoch [10/50], Step [100/ 600], Loss: 0.1802\n",
      "Epoch [10/50], Step [200/ 600], Loss: 0.0301\n",
      "Epoch [10/50], Step [300/ 600], Loss: 0.0520\n",
      "Epoch [10/50], Step [400/ 600], Loss: 0.0594\n",
      "Epoch [10/50], Step [500/ 600], Loss: 0.0910\n",
      "Epoch [10/50], Step [600/ 600], Loss: 0.0912\n",
      "Epoch [11/50], Step [100/ 600], Loss: 0.0177\n",
      "Epoch [11/50], Step [200/ 600], Loss: 0.0310\n",
      "Epoch [11/50], Step [300/ 600], Loss: 0.0976\n",
      "Epoch [11/50], Step [400/ 600], Loss: 0.1993\n",
      "Epoch [11/50], Step [500/ 600], Loss: 0.1344\n",
      "Epoch [11/50], Step [600/ 600], Loss: 0.1997\n",
      "Epoch [12/50], Step [100/ 600], Loss: 0.1888\n",
      "Epoch [12/50], Step [200/ 600], Loss: 0.0301\n",
      "Epoch [12/50], Step [300/ 600], Loss: 0.0661\n",
      "Epoch [12/50], Step [400/ 600], Loss: 0.2399\n",
      "Epoch [12/50], Step [500/ 600], Loss: 0.1496\n",
      "Epoch [12/50], Step [600/ 600], Loss: 0.2568\n",
      "Epoch [13/50], Step [100/ 600], Loss: 0.2834\n",
      "Epoch [13/50], Step [200/ 600], Loss: 0.2159\n",
      "Epoch [13/50], Step [300/ 600], Loss: 0.2721\n",
      "Epoch [13/50], Step [400/ 600], Loss: 0.1860\n",
      "Epoch [13/50], Step [500/ 600], Loss: 0.7943\n",
      "Epoch [13/50], Step [600/ 600], Loss: 0.6112\n",
      "Epoch [14/50], Step [100/ 600], Loss: 0.5563\n",
      "Epoch [14/50], Step [200/ 600], Loss: 0.2780\n",
      "Epoch [14/50], Step [300/ 600], Loss: 0.3554\n",
      "Epoch [14/50], Step [400/ 600], Loss: 0.5582\n",
      "Epoch [14/50], Step [500/ 600], Loss: 0.3795\n",
      "Epoch [14/50], Step [600/ 600], Loss: 0.4171\n",
      "Epoch [15/50], Step [100/ 600], Loss: 0.2641\n",
      "Epoch [15/50], Step [200/ 600], Loss: 0.3703\n",
      "Epoch [15/50], Step [300/ 600], Loss: 0.1851\n",
      "Epoch [15/50], Step [400/ 600], Loss: 0.1724\n",
      "Epoch [15/50], Step [500/ 600], Loss: 0.3694\n",
      "Epoch [15/50], Step [600/ 600], Loss: 0.2357\n",
      "Epoch [16/50], Step [100/ 600], Loss: 0.3838\n",
      "Epoch [16/50], Step [200/ 600], Loss: 0.4228\n",
      "Epoch [16/50], Step [300/ 600], Loss: 0.2770\n",
      "Epoch [16/50], Step [400/ 600], Loss: 0.1476\n",
      "Epoch [16/50], Step [500/ 600], Loss: 0.3913\n",
      "Epoch [16/50], Step [600/ 600], Loss: 0.2870\n",
      "Epoch [17/50], Step [100/ 600], Loss: 0.1204\n",
      "Epoch [17/50], Step [200/ 600], Loss: 0.2691\n",
      "Epoch [17/50], Step [300/ 600], Loss: 0.2105\n",
      "Epoch [17/50], Step [400/ 600], Loss: 0.8008\n",
      "Epoch [17/50], Step [500/ 600], Loss: 0.5602\n",
      "Epoch [17/50], Step [600/ 600], Loss: 0.7521\n",
      "Epoch [18/50], Step [100/ 600], Loss: 0.5407\n",
      "Epoch [18/50], Step [200/ 600], Loss: 0.5173\n",
      "Epoch [18/50], Step [300/ 600], Loss: 0.5595\n",
      "Epoch [18/50], Step [400/ 600], Loss: 0.3923\n",
      "Epoch [18/50], Step [500/ 600], Loss: 0.3529\n",
      "Epoch [18/50], Step [600/ 600], Loss: 0.2840\n",
      "Epoch [19/50], Step [100/ 600], Loss: 0.3385\n",
      "Epoch [19/50], Step [200/ 600], Loss: 0.2284\n",
      "Epoch [19/50], Step [300/ 600], Loss: 0.4012\n",
      "Epoch [19/50], Step [400/ 600], Loss: 0.2448\n",
      "Epoch [19/50], Step [500/ 600], Loss: 0.4798\n",
      "Epoch [19/50], Step [600/ 600], Loss: 0.2530\n",
      "Epoch [20/50], Step [100/ 600], Loss: 0.2780\n",
      "Epoch [20/50], Step [200/ 600], Loss: 0.1972\n",
      "Epoch [20/50], Step [300/ 600], Loss: 0.2220\n",
      "Epoch [20/50], Step [400/ 600], Loss: 0.1607\n",
      "Epoch [20/50], Step [500/ 600], Loss: 0.2752\n",
      "Epoch [20/50], Step [600/ 600], Loss: 0.2180\n",
      "Epoch [21/50], Step [100/ 600], Loss: 0.1501\n",
      "Epoch [21/50], Step [200/ 600], Loss: 0.1845\n",
      "Epoch [21/50], Step [300/ 600], Loss: 0.1971\n",
      "Epoch [21/50], Step [400/ 600], Loss: 0.2113\n",
      "Epoch [21/50], Step [500/ 600], Loss: 0.1139\n",
      "Epoch [21/50], Step [600/ 600], Loss: 0.2324\n",
      "Epoch [22/50], Step [100/ 600], Loss: 0.2337\n",
      "Epoch [22/50], Step [200/ 600], Loss: 0.2794\n",
      "Epoch [22/50], Step [300/ 600], Loss: 0.1715\n",
      "Epoch [22/50], Step [400/ 600], Loss: 0.2562\n",
      "Epoch [22/50], Step [500/ 600], Loss: 0.9326\n",
      "Epoch [22/50], Step [600/ 600], Loss: 0.4933\n",
      "Epoch [23/50], Step [100/ 600], Loss: 0.5974\n",
      "Epoch [23/50], Step [200/ 600], Loss: 0.6009\n",
      "Epoch [23/50], Step [300/ 600], Loss: 0.7179\n",
      "Epoch [23/50], Step [400/ 600], Loss: 0.5901\n",
      "Epoch [23/50], Step [500/ 600], Loss: 0.7987\n",
      "Epoch [23/50], Step [600/ 600], Loss: 0.5498\n",
      "Epoch [24/50], Step [100/ 600], Loss: 0.3587\n",
      "Epoch [24/50], Step [200/ 600], Loss: 0.2034\n",
      "Epoch [24/50], Step [300/ 600], Loss: 0.1803\n",
      "Epoch [24/50], Step [400/ 600], Loss: 0.2284\n",
      "Epoch [24/50], Step [500/ 600], Loss: 0.1651\n",
      "Epoch [24/50], Step [600/ 600], Loss: 0.3069\n",
      "Epoch [25/50], Step [100/ 600], Loss: 0.1849\n",
      "Epoch [25/50], Step [200/ 600], Loss: 0.1076\n",
      "Epoch [25/50], Step [300/ 600], Loss: 0.2198\n",
      "Epoch [25/50], Step [400/ 600], Loss: 0.1494\n",
      "Epoch [25/50], Step [500/ 600], Loss: 0.1502\n",
      "Epoch [25/50], Step [600/ 600], Loss: 0.1879\n",
      "Epoch [26/50], Step [100/ 600], Loss: 0.1468\n",
      "Epoch [26/50], Step [200/ 600], Loss: 0.1110\n",
      "Epoch [26/50], Step [300/ 600], Loss: 0.2105\n",
      "Epoch [26/50], Step [400/ 600], Loss: 0.2070\n",
      "Epoch [26/50], Step [500/ 600], Loss: 0.2221\n",
      "Epoch [26/50], Step [600/ 600], Loss: 0.0280\n",
      "Epoch [27/50], Step [100/ 600], Loss: 0.2187\n",
      "Epoch [27/50], Step [200/ 600], Loss: 0.2272\n",
      "Epoch [27/50], Step [300/ 600], Loss: 0.1585\n",
      "Epoch [27/50], Step [400/ 600], Loss: 0.0590\n",
      "Epoch [27/50], Step [500/ 600], Loss: 0.1226\n",
      "Epoch [27/50], Step [600/ 600], Loss: 0.0991\n",
      "Epoch [28/50], Step [100/ 600], Loss: 0.1252\n",
      "Epoch [28/50], Step [200/ 600], Loss: 0.1775\n",
      "Epoch [28/50], Step [300/ 600], Loss: 0.0893\n",
      "Epoch [28/50], Step [400/ 600], Loss: 0.0980\n",
      "Epoch [28/50], Step [500/ 600], Loss: 0.1579\n",
      "Epoch [28/50], Step [600/ 600], Loss: 0.0669\n",
      "Epoch [29/50], Step [100/ 600], Loss: 0.1436\n",
      "Epoch [29/50], Step [200/ 600], Loss: 0.1118\n",
      "Epoch [29/50], Step [300/ 600], Loss: 0.1969\n",
      "Epoch [29/50], Step [400/ 600], Loss: 0.2240\n",
      "Epoch [29/50], Step [500/ 600], Loss: 0.3867\n",
      "Epoch [29/50], Step [600/ 600], Loss: 0.3865\n",
      "Epoch [30/50], Step [100/ 600], Loss: 0.3199\n",
      "Epoch [30/50], Step [200/ 600], Loss: 0.2906\n",
      "Epoch [30/50], Step [300/ 600], Loss: 0.2684\n",
      "Epoch [30/50], Step [400/ 600], Loss: 0.1853\n",
      "Epoch [30/50], Step [500/ 600], Loss: 0.2258\n",
      "Epoch [30/50], Step [600/ 600], Loss: 0.1997\n",
      "Epoch [31/50], Step [100/ 600], Loss: 0.1949\n",
      "Epoch [31/50], Step [200/ 600], Loss: 0.2397\n",
      "Epoch [31/50], Step [300/ 600], Loss: 0.3143\n",
      "Epoch [31/50], Step [400/ 600], Loss: 0.1330\n",
      "Epoch [31/50], Step [500/ 600], Loss: 0.2099\n",
      "Epoch [31/50], Step [600/ 600], Loss: 0.1210\n",
      "Epoch [32/50], Step [100/ 600], Loss: 0.1599\n",
      "Epoch [32/50], Step [200/ 600], Loss: 0.0684\n",
      "Epoch [32/50], Step [300/ 600], Loss: 0.1305\n",
      "Epoch [32/50], Step [400/ 600], Loss: 0.2574\n",
      "Epoch [32/50], Step [500/ 600], Loss: 0.2318\n",
      "Epoch [32/50], Step [600/ 600], Loss: 0.2395\n",
      "Epoch [33/50], Step [100/ 600], Loss: 0.2432\n",
      "Epoch [33/50], Step [200/ 600], Loss: 0.1374\n",
      "Epoch [33/50], Step [300/ 600], Loss: 0.1119\n",
      "Epoch [33/50], Step [400/ 600], Loss: 0.2038\n",
      "Epoch [33/50], Step [500/ 600], Loss: 0.1743\n",
      "Epoch [33/50], Step [600/ 600], Loss: 0.2450\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for lr, epoch_n in param_combinations:\n",
    "\n",
    "    # Start program timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_hyperparameters = str(lr) +\",\" +str(epoch_n)\n",
    "\n",
    "    results[current_hyperparameters] = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'accuracies': [],\n",
    "        'execution_time': []\n",
    "    }\n",
    "\n",
    "    print(f\"Training with lr={lr}, epoch={epoch_n}\")\n",
    "\n",
    "    # Instantiate and prepare model for training\n",
    "    model = RNNModel(input_dimensions, hidden_unit, rnn_layers, output_classes).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    \n",
    "    # Method for training iterations\n",
    "    def train_model(epoch_count, neural_net, loaders):\n",
    "        for epoch in range(epoch_count):\n",
    "            for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "                # Prep batch data for processing\n",
    "                data = data.view(-1, seq_length, input_dimensions).to(device)\n",
    "                target = target.to(device)\n",
    "    \n",
    "                # Execute a forward pass through the network\n",
    "                predictions = neural_net(data)\n",
    "                loss = loss_function(predictions, target)\n",
    "    \n",
    "                # Compute gradients and adjust model weights\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                results[current_hyperparameters]['train_losses'].append(loss.item())\n",
    "    \n",
    "                # Output training process metrics\n",
    "                if (batch_idx + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch + 1}/{epoch_count}], Step [{batch_idx + 1}/ {len(loaders[\"train\"])}],\n",
    "                    Loss: {loss.item():.4f}')\n",
    "    \n",
    "    # Initiate model training phase\n",
    "    train_model(epoch_n, model, data_loaders)\n",
    "    \n",
    "    \n",
    "    # Method for evaluating network performance\n",
    "    def test_model(neural_net, loaders):\n",
    "        \n",
    "        neural_net.eval()  # Transition model to evaluation mode\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in loaders['test']:\n",
    "                data = data.view(-1, seq_length, input_dimensions).to(device)\n",
    "                target = target.to(device)\n",
    "                predictions = neural_net(data)\n",
    "    \n",
    "                val_loss = loss_function(predictions, target).item()\n",
    "                results[current_hyperparameters]['val_losses'].append(val_loss)\n",
    "    \n",
    "                _, predicted_classes = torch.max(predictions, 1)\n",
    "                correct_predictions += (predicted_classes == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "                \n",
    "        # Calculate overall accuracy after processing all batches\n",
    "        overall_accuracy = 100 * correct_predictions / total_samples\n",
    "        results[current_hyperparameters]['accuracies'].append(overall_accuracy)\n",
    "            \n",
    "        print(f'Test Accuracy of the model on the {total_samples} test images:{100 * correct_predictions / \n",
    "        total_samples:.2f}%')\n",
    "\n",
    "        # End program timer\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        results[current_hyperparameters]['execution_time'].append(total_time)\n",
    "        print(f\"Total execution time: {total_time} seconds\")\n",
    "        \n",
    "    # Execute model evaluation\n",
    "    test_model(model, data_loaders)\n",
    "\n",
    "# Save dictionary to a JSON file\n",
    "with open('RNNresults.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)  # `indent` makes the file human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166bada0-3e0c-4602-b97a-65bcbea7bf81",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"# After all epochs are done (after the training and evaluation loop)\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.title('Training and Validation Loss Over Epochs')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "plt.show()\"\"\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dde71295-f747-4cb4-afb6-da4dac8f0b85",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
