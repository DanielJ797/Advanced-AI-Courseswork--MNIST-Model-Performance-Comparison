{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Comparison of the Performance and Execution Time of Simple Deep Learning Networks on the MNIST\n",
    "By Daniel Jennings (F026987)\n",
    "\n",
    "## Abstract\n",
    "In this tutorial, I will be demonstrating how to build different types of simple deep learning networks and then comparing the performance and execution times of each model to evaluate the usefulness of any given design. To do this, I will provide graphs that will represent the trends in efficiency across each model alongside the relevant data to demonstrate any significant pattern in their execution. Furthermore, each deep learning network type will be accompanied by a step-by-step guide on how to recreate them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Table of Contents\n",
    "- Multilayer Perceptron Tutorial\n",
    "- Convolutional Neural Network Tutorial\n",
    "- Recurrent Neural Network Tutorial\n",
    "- MLP Performance Demonstration\n",
    "- CNN Performance Demonstration\n",
    "- RNN Performance Demonstration\n",
    "- References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Learning Objectives\n",
    "- Create several differnet types of neural network\n",
    "- Evaluate their performance at identifying digits in the MNIST dataset\n",
    "- Compare their performances against each other\n",
    "- Compare their performances against online references\n",
    "- Identify the pros and cons of my models compared to examples online"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multilayer Perceptron (MLP)\n",
    "The first method we will be discussing is a standard multilayer perceptron. This model incorporates a number of feedforward connections between several layers of neurons. It boasts a simple architecture compared to other methods which makes it a perfect design to start with before building on our knowledge with more complex models later on in the tutorial.\r\n",
    "\r\n",
    "Firstly, as with all other models we will import the necessary modules for the program to wkor:\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as network\n",
    "from torch import optim\n",
    "import itertools\n",
    "import time\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next task is to load the data we are going to be working with (the MNIST data set) so that we can train the model. This step is present in every variant of neural network we will use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start program timer\n",
    "start_time = time.time()\n",
    "\n",
    "# Set parameters for data processing\n",
    "num_workers = 0\n",
    "batch_size = 20\n",
    "\n",
    "# Data transformation pipeline\n",
    "data_transforms = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,),\n",
    "                                                                                  (0.5,))])\n",
    "\n",
    "# Load MNIST dataset\n",
    "train_set = datasets.MNIST(root='data_folder', train=True, download=True,\n",
    "                           transform=data_transforms)\n",
    "test_set = datasets.MNIST(root='data_folder', train=False, download=True,\n",
    "                          transform=data_transforms)\n",
    "\n",
    "# Initialize data loaders\n",
    "train_loader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True,\n",
    "                          num_workers=num_workers)\n",
    "test_loader = DataLoader(dataset=test_set, batch_size=batch_size, shuffle=False,\n",
    "                         num_workers=num_workers)\n",
    "\n",
    "# [1] - https://www.kaggle.com/code/fgiorgio/multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once this is completed, we must define the neural network architecture and specify a class with methods that can perform the functionality of a multilayer perceptron. We will define an __init__ function so the model can be referenced and will create the connected layers needed for the network to function.\r\n",
    "\r\n",
    "Following this, we will define the forward method which feeds the input tensor through every connected layer defined in the __init__ function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the neural network architecture\n",
    "class DigitRecognizer(network.Module):\n",
    "    def __init__(self):\n",
    "        super(DigitRecognizer, self).__init__()\n",
    "        self.fc1 = network.Linear(28 * 28, 512)\n",
    "        self.fc2 = network.Linear(512, 512)\n",
    "        self.fc3 = network.Linear(512, 10)\n",
    "        self.dropout = network.Dropout(0.2)\n",
    "\n",
    "    def forward(self, tensor):\n",
    "        tensor = tensor.view(-1, 28 * 28)\n",
    "        tensor = torch.relu(self.fc1(tensor))\n",
    "        tensor = self.dropout(tensor)\n",
    "        tensor = torch.relu(self.fc2(tensor))\n",
    "        tensor = self.dropout(tensor)\n",
    "        tensor = self.fc3(tensor)\n",
    "        return tensor\n",
    "# [1] - https://www.kaggle.com/code/fgiorgio/multi-layer-perceptron-mnist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "Now we must specify the range of learning rates and epoch numbers we will be using so the program can iterate through every combination. Once the ranges have been chosen, we can begin that loop and instantiate the MLP, alongside defining the loss function and optimizer so that the program can reference an MLP object.\n",
    "\n",
    "# Part 2\n",
    "Arguably the most important step, the neural network must now be trained so that it can recognise the pictures of each number. Firstly, this will involve defining the number of epochs and a loop that the program can iterate through to incrementally improve the loss value of the network.\r\n",
    "\r\n",
    "This is achieved using nested loops, the parent loop being used to specify the average loss value and the child loop being used to calculate the training loss for the current epoch. This is demonstrated belo\n",
    "\n",
    "# Part 3\n",
    "The penultimate step will be to test the accuracy of our model on test data that has not been made visible to the neural network before this point. \n",
    "\n",
    "We shall repeat the same process used to train the data but instead use our new test data to identify any discrepancies in the loss value which may result as a result of issues like overtraining or overfitting. Note that we are also recording specific values that are produced in order to produce graphs that can accurately represent the performance of our model.\n",
    "\n",
    "# Part 4\n",
    "Now that both the training and testing functions have been defined, we can then call each method using our customized values for the learning rate and epochs, this will effectively begin executing the model. We also record the execution time of the whole program and export all the relevant data to an external .json file which will be using later.\n",
    ":\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------PART 1-----------------------------------------\n",
    "learning_rates = [0.01, 0.001, 0.0005]\n",
    "num_epochs = [10, 20, 50]\n",
    "param_combinations = list(itertools.product(learning_rates, num_epochs))\n",
    "\n",
    "results = {}\n",
    "\n",
    "for lr, epoch_n in param_combinations:\n",
    "    print(f\"Training with lr={lr}, epoch={epoch_n}\")\n",
    "\n",
    "    # Set the loss function and optimizer\n",
    "    digit_recognizer = DigitRecognizer()\n",
    "    loss_function = network.CrossEntropyLoss()\n",
    "    optimizer = optim.SGD(digit_recognizer.parameters(), lr=lr)\n",
    "\n",
    "    current_hyperparameters = str(lr) +\",\" +str(epoch_n)\n",
    "    results[current_hyperparameters] = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'accuracies': [],\n",
    "        'execution_time': []\n",
    "    }\n",
    "\n",
    "    #---------------------------------PART 2-----------------------------------------\n",
    "\n",
    "    # Define the training process\n",
    "    def train_network(epochs, model, loader):\n",
    "        for epoch in range(epochs):\n",
    "            running_loss = 0.0\n",
    "            for images, labels in loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = model(images)\n",
    "                loss = loss_function(outputs, labels)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                results[current_hyperparameters]['train_losses'].append(loss.item())\n",
    "                running_loss += loss.item()\n",
    "    \n",
    "            print(f'Epoch {epoch + 1} complete: Avg. Loss: {running_loss / len(loader)}')\n",
    "\n",
    "    #---------------------------------PART 3-----------------------------------------\n",
    "    \n",
    "    # Define the testing process\n",
    "    def test_network(model, loader):\n",
    "        \n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        class_correct = list(0. for i in range(10))\n",
    "        class_total = list(0. for i in range(10))\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in loader:\n",
    "                outputs = model(inputs)\n",
    "\n",
    "                val_loss = loss_function(outputs, labels).item()\n",
    "                results[current_hyperparameters]['val_losses'].append(val_loss)\n",
    "            \n",
    "                loss = loss_function(outputs, labels)\n",
    "                _, predicted = torch.max(outputs, 1)\n",
    "                total_correct += (predicted == labels).sum().item()\n",
    "                total_samples += labels.size(0)\n",
    "                c = (predicted == labels).squeeze()\n",
    "                for i in range(batch_size):\n",
    "                    label = labels[i]\n",
    "                    class_correct[label] += c[i].item()\n",
    "                    class_total[label] += 1\n",
    "\n",
    "        accuracy = 100 * total_correct / total_samples\n",
    "        results[current_hyperparameters]['accuracies'].append(accuracy)\n",
    "        print(f'Test accuracy: {100 * total_correct / total_samples}%')\n",
    "        for i in range(10):\n",
    "            print(f'Accuracy of digit {i}: {100 * class_correct[i] / class_total[i]}%')\n",
    "\n",
    "    #---------------------------------PART 4-----------------------------------------\n",
    "    \n",
    "    # Training and testing\n",
    "    train_network(epoch_n, digit_recognizer, train_loader)\n",
    "    test_network(digit_recognizer, test_loader)\n",
    "\n",
    "    # End program timer\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    results[current_hyperparameters]['execution_time'].append(total_time)\n",
    "    print(f\"Total execution time: {total_time} seconds\")\n",
    "\n",
    "\n",
    "# Save results to a JSON file\n",
    "with open('MLPresults.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolutional Neural Network (CNN)\n",
    "The second layer we will be discussing is the Convolutional Neural Network (CNN), this is more commonly used to perform image recognition on data sets like MNIST because it is so well suited to capture spatial information when compared to other designs such as a multilayer perceptron. The following section will be dedicated to showing how to build your own convolutional neural network using pytorch.\r\n",
    "\r\n",
    "Firstly we must import the necessary modules which contain the methods that will enable us to create our neural networ.\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we must check if CUDA is available in order to successfully run the neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for CUDA and use it if available, else use CPU\n",
    "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {compute_device} device for computation.')\n",
    "#[2] - https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Like before, the next step is to import the MNIST data so it can be used to train and test the model alongside the hyperparameter ranges we want to test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the MNIST dataset for training and testing\n",
    "training_dataset = datasets.MNIST(\n",
    "    root='./dataset_storage',\n",
    "    train=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "testing_dataset = datasets.MNIST(\n",
    "    root='./dataset_storage',\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")\n",
    "#[2] - https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for batching and shuffling the datasets\n",
    "data_loaders = {\n",
    "    'train_loader': DataLoader(training_dataset, batch_size=100, shuffle=True, num_workers=2),\n",
    "    'test_loader': DataLoader(testing_dataset, batch_size=100, shuffle=True, num_workers=2)\n",
    "}\n",
    "\n",
    "# [2] - https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005]\n",
    "num_epochs = [10, 20, 50]\n",
    "\n",
    "param_combinations = list(itertools.product(learning_rates, num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we want to define the starting function __init__ with 2 convolutional layers, both of these layers are succeeded by rectifier activation functions and max pooling. The output of this function will be a connected layer which assigns individual class scores to any identifiable features the neural network can find.\n",
    "\n",
    "From this we can then build our forward_pass function which takes a tensor (we will call ‘x’ ) and passes it through the layers we created in the __init__ function to yield an \r\n",
    "output value\r\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network Architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Defining layers in the network\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.dense = nn.Linear(32*7*7, 10)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        input_data = self.layer1(input_data)\n",
    "        input_data = self.layer2(input_data)\n",
    "        input_data = input_data.view(input_data.size(0), -1)  # Flatten the tensor\n",
    "        return self.dense(input_data)\n",
    "\n",
    "#[2] - https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "The next step involves beginning the loop which will iterate through every possible combination of hyperparameters we want and defining the dictionary which we will use to store any relevant values that will be generated during the training/testing process. Following this, we will instantiate the neural network, loss function and optimization function.\n",
    "\n",
    "# Part 2\n",
    "Once this is completed we can begin to train the neural network by applying many of the same techniques we used when designing the Multilayer Perceptron. This includes a loop that specifies a new epoch with each iteration.\n",
    "\n",
    "# part 3\n",
    "We can then test the model by once again using the model.eval() method, this is done using the following code. Please note what values are being recorded while we test/train\n",
    "\n",
    "# Part 4\n",
    "Now that both the training and testing functions have been defined, we can then call each method using our customized values for the learning rate and epochs, this will effectively begin executing the model. We also record the execution time of the whole program and export all the relevant data to an external .json file which will be using later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------PART 1-----------------------------------------\n",
    "results = {}\n",
    "\n",
    "for lr, epoch_n in param_combinations:\n",
    "\n",
    "    # Start program timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_hyperparameters = str(lr)+\",\"+str(epoch_n)\n",
    "\n",
    "    results[current_hyperparameters] = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'accuracies': [],\n",
    "        'execution_time': []\n",
    "    }\n",
    "\n",
    "    print(f\"Training with lr={lr}, epoch={epoch_n}\")\n",
    "    \n",
    "    # Instantiate the network, loss function and optimizer\n",
    "    net = CNN().to(compute_device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "\n",
    "    #---------------------------------PART 2-----------------------------------------\n",
    "    \n",
    "    # Training Procedure\n",
    "    def train_model(epochs, network, loaders):\n",
    "        network.train()  # Set the network to training mode\n",
    "    \n",
    "        for e in range(epochs):\n",
    "            for batch_idx, (inputs, targets) in enumerate(loaders['train_loader']):\n",
    "                inputs, targets = inputs.to(compute_device), targets.to(compute_device)\n",
    "                network.zero_grad()\n",
    "                outputs = network.forward_pass(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                results[current_hyperparameters]['train_losses'].append(loss.item())\n",
    "    \n",
    "                if batch_idx % 100 == 0:\n",
    "                    print(f'Epoch {e+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    \n",
    "        print(\"Training complete.\")\n",
    "    \n",
    "    train_model(epoch_n, net, data_loaders)\n",
    "\n",
    "    #---------------------------------PART 3-----------------------------------------\n",
    "    \n",
    "    # Function to evaluate the model performance on the test dataset\n",
    "    def evaluate_model(network, loaders):\n",
    "        network.eval()  # Set the network to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():  # No need to track gradients for validation\n",
    "            for inputs, targets in loaders['test_loader']:\n",
    "                inputs, targets = inputs.to(compute_device), targets.to(compute_device)\n",
    "                outputs = network.forward_pass(inputs)\n",
    "\n",
    "                val_loss = criterion(outputs, targets).item()\n",
    "                results[current_hyperparameters]['val_losses'].append(val_loss)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "    \n",
    "        accuracy = 100 * correct / total\n",
    "        results[current_hyperparameters]['accuracies'].append(accuracy)\n",
    "        print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    \n",
    "    # Evaluate the trained model\n",
    "    evaluate_model(net, data_loaders)\n",
    "\n",
    "    #---------------------------------PART 4-----------------------------------------\n",
    "    \n",
    "    # End program timer\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    results[current_hyperparameters]['execution_time'].append(total_time)\n",
    "    print(f\"Total execution time: {total_time} seconds\")\n",
    "\n",
    "# Save dictionary to a JSON file\n",
    "with open('CNNresults.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)  # `indent` makes the file human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recurrent Neural Network (RNN)\n",
    "\n",
    "A recurrent Neural network is a model not typically used for image data like the MNIST data set and is generally employed for more sequential data like text or time series. However, it still provides an effective model which we can use to compare the performance and execution of other designs so that we can more accurately evaluate which are better as a whole.\n",
    "\n",
    "As with all other designs, the first step involves importing the necessary modules to be used later in the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from torch import optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "import time\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must then check that CUDA is available to perform the training and testing of any neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup computational device based on CUDA availability\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "#[3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then, once again just like the other models, we will import the MNIST data and then prepare the loaders so that the data can actually be used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Acquire and organize MNIST training dataset\n",
    "train_dataset = datasets.MNIST(root='data', train=True, transform=transforms.ToTensor(),\n",
    "                               download=True)\n",
    "train_loader = DataLoader(train_dataset, batch_size=100, shuffle=True, num_workers=1)\n",
    "\n",
    "# Acquire and organize MNIST testing dataset\n",
    "test_dataset = datasets.MNIST(root='data', train=False, transform=transforms.ToTensor())\n",
    "test_loader = DataLoader(test_dataset, batch_size=100, shuffle=False, num_workers=1)\n",
    "\n",
    "# Group data loaders into a dictionary for ease of access\n",
    "data_loaders = {'train': train_loader, 'test': test_loader}\n",
    "#[3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once the preliminary steps, we must instantiate the hyper parameters to finely tune the model. This will include defining the range of parameters we want to change"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define hyperparameters\n",
    "seq_length = 28\n",
    "input_dimensions = 28\n",
    "rnn_layers = 2\n",
    "output_classes = 10\n",
    "batch_sz = 100\n",
    "hidden_unit = 128\n",
    "\n",
    "#[3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005]\n",
    "num_epochs = [10, 20, 50]\n",
    "\n",
    "param_combinations = list(itertools.product(learning_rates, num_epochs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we must define the constructor method, like both of the other models: this will be used to define the layers of the neural network alongside the input dimensions and the number of possible classes any input could be placed into.\n",
    "\n",
    "Once this is done, we then define the forward method which will initialize the cell state and hidden state for the long short-term memory for the first cell in the neural network. Following this, we can then pass any input (denoted by x) and the hidden state into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct RNN architecture\n",
    "class RNNModel(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_units, rnn_layers, output_classes):\n",
    "        super(RNNModel, self).__init__()\n",
    "        self.hidden_units = hidden_units\n",
    "        self.rnn_layers = rnn_layers\n",
    "        self.rnn = nn.LSTM(input_dim, hidden_units, rnn_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_units, output_classes)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Initialize hidden and cell states for LSTM layers\n",
    "        h0 = torch.zeros(self.rnn_layers, x.size(0), self.hidden_units).to(device)\n",
    "        c0 = torch.zeros(self.rnn_layers, x.size(0), self.hidden_units).to(device)\n",
    "        \n",
    "        # Feed data through recurrent layers and obtain last output\n",
    "        out, _ = self.rnn(x, (h0, c0))  # Tuple of (hidden state, cell state)\n",
    "        \n",
    "        # Adapt the output for the final classification layer\n",
    "        out = self.fc(out[:, -1, :])  # Get the last time step output for each batch\n",
    "        return out\n",
    "\n",
    "#[3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "We must now begin the loop which will check every combination of hyperparameters, define the values of interest we want to store and instantiate the model, loss function and optimizer.\n",
    "\n",
    "# Part 2\n",
    "The model must now be trained, which will once again be done using a nested for loop where the program will iterate through each epoch in an attempt to reduce the loss value.It does this by calculating the difference between the actual output and the predicted output as a numerical value and then backpropagating to calculate new gradients to be used in the next iteration. \n",
    "\n",
    "# Part 3\n",
    "The accuracy of the model must now be tested using the following code by simply finding the average difference between the actual and predicted outcomes. We will also continue to extract the desired values we want to analyze later\n",
    "\n",
    "# Part 4\n",
    "Finally, we can export all the relevant training and testing data to an external .json file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#---------------------------------PART 1-----------------------------------------\n",
    "results = {}\n",
    "\n",
    "for lr, epoch_n in param_combinations:\n",
    "\n",
    "    # Start program timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_hyperparameters = str(lr) +\",\" +str(epoch_n)\n",
    "\n",
    "    results[current_hyperparameters] = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'accuracies': [],\n",
    "        'execution_time': []\n",
    "    }\n",
    "\n",
    "    print(f\"Training with lr={lr}, epoch={epoch_n}\")\n",
    "\n",
    "    # Instantiate and prepare model for training\n",
    "    model = RNNModel(input_dimensions, hidden_unit, rnn_layers, output_classes).to(device)\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    #---------------------------------PART 2-----------------------------------------\n",
    "    \n",
    "    # Method for training iterations\n",
    "    def train_model(epoch_count, neural_net, loaders):\n",
    "        for epoch in range(epoch_count):\n",
    "            for batch_idx, (data, target) in enumerate(loaders['train']):\n",
    "                # Prep batch data for processing\n",
    "                data = data.view(-1, seq_length, input_dimensions).to(device)\n",
    "                target = target.to(device)\n",
    "    \n",
    "                # Execute a forward pass through the network\n",
    "                predictions = neural_net(data)\n",
    "                loss = loss_function(predictions, target)\n",
    "    \n",
    "                # Compute gradients and adjust model weights\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "    \n",
    "                results[current_hyperparameters]['train_losses'].append(loss.item())\n",
    "    \n",
    "                # Output training process metrics\n",
    "                if (batch_idx + 1) % 100 == 0:\n",
    "                    print(f'Epoch [{epoch + 1}/{epoch_count}], Step [{batch_idx + 1}/ {len(loaders[\"train\"])}],\n",
    "                    Loss: {loss.item():.4f}')\n",
    "\n",
    "                # [3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f\n",
    "    \n",
    "    # Initiate model training phase\n",
    "    train_model(epoch_n, model, data_loaders)\n",
    "\n",
    "    #---------------------------------PART 3-----------------------------------------\n",
    "    \n",
    "    # Method for evaluating network performance\n",
    "    def test_model(neural_net, loaders):\n",
    "        \n",
    "        neural_net.eval()  # Transition model to evaluation mode\n",
    "        total_samples = 0\n",
    "        correct_predictions = 0\n",
    "        with torch.no_grad():\n",
    "            for data, target in loaders['test']:\n",
    "                data = data.view(-1, seq_length, input_dimensions).to(device)\n",
    "                target = target.to(device)\n",
    "                predictions = neural_net(data)\n",
    "    \n",
    "                val_loss = loss_function(predictions, target).item()\n",
    "                results[current_hyperparameters]['val_losses'].append(val_loss)\n",
    "    \n",
    "                _, predicted_classes = torch.max(predictions, 1)\n",
    "                correct_predictions += (predicted_classes == target).sum().item()\n",
    "                total_samples += target.size(0)\n",
    "                \n",
    "        # Calculate overall accuracy after processing all batches\n",
    "        overall_accuracy = 100 * correct_predictions / total_samples\n",
    "        results[current_hyperparameters]['accuracies'].append(overall_accuracy)\n",
    "            \n",
    "        print(f'Test Accuracy of the model on the {total_samples} test images:{100 * correct_predictions / \n",
    "        total_samples:.2f}%')\n",
    "\n",
    "        # [3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f\n",
    "\n",
    "        # End program timer\n",
    "        end_time = time.time()\n",
    "        total_time = end_time - start_time\n",
    "        results[current_hyperparameters]['execution_time'].append(total_time)\n",
    "        print(f\"Total execution time: {total_time} seconds\")\n",
    "        \n",
    "    # Execute model evaluation\n",
    "    test_model(model, data_loaders)\n",
    "\n",
    "#---------------------------------PART 4-----------------------------------------\n",
    "\n",
    "# Save dictionary to a JSON file\n",
    "with open('RNNresults.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)  # `indent` makes the file human-readable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP performance demonstration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "|Learning Rate |Number of epochs|Accuracy  |Execution Time | \n",
    "|-----|:-----|:---:|:-----:|\n",
    "|0.01 |10  | 96.96     |  138.8  |\n",
    "|0.01| 20|   97.68   |   409.9  |\n",
    "|0.01 |  50|  97.91    | 1043.1   |\n",
    "|0.001 |10  | 91.69     | 1172   |\n",
    "|0.001 |20  | 94.05     |  1427.1  |\n",
    "|0.001 |50  |  96.45   |  2063.8  |\n",
    "|0.0005 |10  |   89.45   |  2191.8  |\n",
    "|0.0005 |20  |  91.73    |  2446.8  |\n",
    "|0.0005 |50  |   94.71   | 3081.2    |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# CNN performance demonstration\n",
    "\n",
    "|Learning Rate |Number of epochs|Accuracy (%) |Execution Time (seconds)| \n",
    "|-----|:-----|:---:|:-----:|\n",
    "|0.01 |  10| 97.77  |    74.9|\n",
    "|0.01 |  20|   98.41|    149.1|\n",
    "|0.01 |  50|   98.9|  392.6  |\n",
    "|0.001 |  10|99.11   | 77.5   |\n",
    "|0.001 |  20|99.12   |  151.2  |\n",
    "|0.001 |  50|  99.12 |  380.3  |\n",
    "|0.0005 |  10|  99.03 |  76.6  |\n",
    "|0.0005 |  20|  99.14 |  152.6  |\n",
    "|0.0005 |  50| 99.08  |  381.1  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RNN performance demonstration\n",
    "|Learning Rate |Number of epochs|Accuracy (%) |Execution Time (seconds)| \n",
    "|-----|:-----|:---:|:-----:|\n",
    "|0.01 |  10| 97.39  |  166.1  |\n",
    "|0.01 |  20| 97.12  | 335.8   |\n",
    "|0.01 |  50|  96.64 |  1324.6  |\n",
    "|0.001 |  10|  98.67 |  168.4  |\n",
    "|0.001 |  20|  98.69 | 332.8   |\n",
    "|0.001 |  50| 99.14 | 835   |\n",
    "|0.0005 |  10| 98.41|  168.6  |\n",
    "|0.0005 |  20| 98.85|  333.9  |\n",
    "|0.0005 |  50| 99.06|  832.3  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Graphical representations of each models performance\n",
    "Once you have successfully completed executing one of the neural network models above, run the code segment corresponding to the model you want to evaluate (A, B or C) and then execute the graph generator to get a visual representation of each model's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Multilayer Perceptron (A) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve training results from the specified JSON file\n",
    "results_file = 'MLPresults.json'\n",
    "with open(results_file, 'r') as file_handle:\n",
    "    experiment_data = json.load(file_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Convolutional Neural Network (B) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve training results from the specified JSON file\n",
    "results_file = 'CNNresults.json'\n",
    "with open(results_file, 'r') as file_handle:\n",
    "    experiment_data = json.load(file_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Recurrent neural Network (C)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Retrieve training results from the specified JSON file\n",
    "results_file = 'RNNresults.json'\n",
    "with open(results_file, 'r') as file_handle:\n",
    "    experiment_data = json.load(file_handle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph generator code\n",
    "Run this once you have impotred a JSON file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up a mapping to track the accumulated accuracies per epoch\n",
    "epoch_accuracy_aggregate = {10: [], 20: [], 50: []}\n",
    "\n",
    "# Gather the final accuracy from each experiment configuration\n",
    "for configuration, metrics in experiment_data.items():\n",
    "    rate, epoch_marker = configuration.split(',')\n",
    "    epoch_marker = int(epoch_marker)\n",
    "    \n",
    "    if epoch_marker in epoch_accuracy_aggregate:\n",
    "        # Deal with possible missing accuracy data\n",
    "        final_accuracies = metrics.get('accuracies', [])\n",
    "        if final_accuracies:\n",
    "            epoch_accuracy_aggregate[epoch_marker].append(final_accuracies[-1])\n",
    "\n",
    "# Average out the accuracies for each specified epoch\n",
    "for epoch_marker in epoch_accuracy_aggregate:\n",
    "    accuracy_list = epoch_accuracy_aggregate[epoch_marker]\n",
    "    epoch_accuracy_aggregate[epoch_marker] = sum(accuracy_list) / len(accuracy_list) if accuracy_list else None\n",
    "\n",
    "# Create a visual representation of the accuracy averages\n",
    "plot_figure, plot_axis = plt.subplots()\n",
    "plot_axis.plot(epoch_accuracy_aggregate.keys(), epoch_accuracy_aggregate.values(), marker='o', linestyle='-')\n",
    "plot_axis.set_xlabel('Epoch Count')\n",
    "plot_axis.set_ylabel('Mean Accuracy')\n",
    "plot_axis.set_title('Mean Accuracy Per Epoch Count')\n",
    "plot_axis.grid(visible=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons of my MLP implementation compared to references\n",
    "## Pros:\n",
    "- Performance analytics: My implementation records several values that are generated by the program in order to measure its performance as a whole. This is not available in many online tutuorials and demonstrates a visible advantage of my model\r",
    "- Modular: testing and training are seperated into self-contained method. This improves elements like debugging and security\n",
    "- Scalable - New methods can be added at any time to add new functionality which can then be easily referenced from anywhere within the program\n",
    "- Regularization: I also use dropout regularization in my code to stop overfitting from happening, which improves the      performance of the model\n",
    "- Hyper-parameters: My model offers a range of hyperparameter values to use which allows the user to find the ihghest performing configuration\n",
    "\n",
    "## Cons\n",
    "- Complexity: Because of the focus on using different hyperparameter combinations, the code is far more complex than many tutorials online as straightforwards processes are elongated\n",
    "- Longer Execution Time: Again because of the focus on hyperparameter combinations, the execution time of the program is far larger than most programs available online\n",
    "- Computationally Expensive: focusing on producing such a wide range of results consumes a large amount of resources and limits the mmodel's usability on less powerful computersel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pros and Cons of my CNN implementation compared to references\n",
    "## Pros:\n",
    "- Improved performance: my model has an average acccuracy of 98.9% across all configurations, this is generally higher than the average accuracy found in online tutorials, using a learning rate of 0.001 even offers an accuracy of 99.12\n",
    "- Performance analytics: My implementation records several values that are generated by the program in order to measure its performance as a whole. This is not available in many online tutuorials and demonstrates a visible advantage of my model\n",
    "- Regularization: I also use dropout regularization in my code to stop overfitting from happening, which improves the      performance of the model\n",
    "- Hyper-parameters: My model offers a range of hyperparameter values to use which allows the user to find the ihghest performing configuration\n",
    "- Automatic Results saving mechanism: Saving all the relevant statistics in a json file at the end of the program enables easy access to large amounts of useful performance data and makes it easier to analyse and compare to other models\n",
    "\n",
    "## Cons\n",
    "- Complexity: Because of the focus on using different hyperparameter combinations, the code is far more complex than many tutorials online as straightforwards processes are elongated\n",
    "- Longer Execution Time: Again because of the focus on hyperparameter combinations, the execution time of the program is far larger than most programs available online\n",
    "- Computationally Expensive: focusing on producing such a wide range of results consumes a large amount of resources and limits the mmodel's usability on less powerful computers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# Pros and Cons of my RNN implementation compared to references\n",
    "## Pros:\n",
    "- Performance analytics: My implementation records several values that are generated by the program in order to measure its performance as a whole. This is not available in many online tutuorials and demonstrates a visible advantage of my model\n",
    "- Regularization: I also use dropout regularization in my code to stop overfitting from happening, which improves the      performance of the model\n",
    "- Hyper-parameters: My model offers a range of hyperparameter values to use which allows the user to find the ihghest performing configuration\n",
    "- Automatic Results saving mechanism: Saving all the relevant statistics in a json file at the end of the program enables easy access to large amounts of useful performance data and makes it easier to analyse and compare to other models\n",
    "\n",
    "## Cons\n",
    "- Complexity: Because of the focus on using different hyperparameter combinations, the code is far more complex than many tutorials online as straightforwards processes are elongated\n",
    "- Longer Execution Time: Again because of the focus on hyperparameter combinations, the execution time of the program is far larger than most programs available online\n",
    "- Computationally Expensive: focusing on producing such a wide range of results consumes a large amount of resources and limits the mmodel's usability on less powerful computers\n",
    "- Less Visual information: Many tutorials provide visualisations of the digits but I have foregone this improve execution time, however it is less visually appealing for the user"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "# References\n",
    "\n",
    "- [1] - https://www.kaggle.com/code/fgiorgio/multi-layer-perceptron-mnist\n",
    "- [2] - https://medium.com/@nutanbhogendrasharma/pytorch-convolutional-neural-network-with-mnist-dataset-4e8a4265e118\n",
    "- [3] - https://medium.com/@nutanbhogendrasharma/pytorch-recurrent-neural-networks-with-mnist-dataset-2195033b540f\n",
    "- [4] - https://medium.com/analytics-vidhya/multi-layer-perceptron-using-keras-on-mnist-dataset-for-digit-classification-problem-relu-a276cbf05e97\n",
    "- [5] - https://saltfarmer.github.io/blog/machine%20learning/deep%20learning/MNIST-with-Multi-Layer-Perceptron/\n",
    "- [6] - https://www.geeksforgeeks.org/applying-convolutional-neural-network-on-mnist-dataset/\n",
    "- [7] - https://machinelearningmastery.com/how-to-develop-a-convolutional-neural-network-from-scratch-for-mnist-handwritten-digit-classification/\n",
    "- [8] - https://www.tensorflow.org/guide/keras/working_with_rnns\n",
    "- [9] - https://medium.com/the-artificial-impostor/notes-understanding-tensorflow-part-2-f7e5ece849f5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
