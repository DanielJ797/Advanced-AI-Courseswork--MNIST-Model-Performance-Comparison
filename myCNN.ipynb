{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "45ff3f3a-186c-496e-b737-6bf59466f3e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torchvision import transforms,datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "from torch.autograd import Variable\n",
    "import time\n",
    "import itertools\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e6d3a3a-61d3-4286-a86d-1e9fe23a44db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device for computation.\n"
     ]
    }
   ],
   "source": [
    "# Check for CUDA and use it if available, else use CPU\n",
    "compute_device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Using {compute_device} device for computation.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "21aadb91-91b0-4ca9-b114-390039a94174",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and prepare the MNIST dataset for training and testing\n",
    "training_dataset = datasets.MNIST(\n",
    "    root='./dataset_storage',\n",
    "    train=True,\n",
    "    transform=transforms.Compose([transforms.ToTensor()]),\n",
    "    download=True\n",
    ")\n",
    "\n",
    "testing_dataset = datasets.MNIST(\n",
    "    root='./dataset_storage',\n",
    "    train=False,\n",
    "    transform=transforms.Compose([transforms.ToTensor()])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1427bfde-7c3e-4011-bf93-594a43e35662",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaders for batching and shuffling the datasets\n",
    "data_loaders = {\n",
    "    'train_loader': DataLoader(training_dataset, batch_size=100, shuffle=True, num_workers=2),\n",
    "    'test_loader': DataLoader(testing_dataset, batch_size=100, shuffle=True, num_workers=2)\n",
    "}\n",
    "\n",
    "learning_rates = [0.01, 0.001, 0.0005]\n",
    "num_epochs = [10, 20, 50]\n",
    "\n",
    "param_combinations = list(itertools.product(learning_rates, num_epochs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbc58e25-0fe8-473f-8dee-f29b0b9c0bfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the Neural Network Architecture\n",
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN, self).__init__()\n",
    "        # Defining layers in the network\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 16, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(16, 32, 5, 1, 2),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(2)\n",
    "        )\n",
    "        self.dense = nn.Linear(32*7*7, 10)\n",
    "\n",
    "    def forward_pass(self, input_data):\n",
    "        input_data = self.layer1(input_data)\n",
    "        input_data = self.layer2(input_data)\n",
    "        input_data = input_data.view(input_data.size(0), -1)  # Flatten the tensor\n",
    "        return self.dense(input_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0e435853-4ae7-488e-bb99-4945fc637f8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=0.01, epoch=1\n",
      "Epoch 1/1, Batch 0, Loss: 2.3010306358337402\n",
      "Epoch 1/1, Batch 100, Loss: 0.15400080382823944\n",
      "Epoch 1/1, Batch 200, Loss: 0.12409922480583191\n",
      "Epoch 1/1, Batch 300, Loss: 0.06551418453454971\n",
      "Epoch 1/1, Batch 400, Loss: 0.13558310270309448\n",
      "Epoch 1/1, Batch 500, Loss: 0.023453976958990097\n",
      "Training complete.\n",
      "0.04513672739267349\n",
      "0.10174111276865005\n",
      "0.04545227065682411\n",
      "0.048181869089603424\n",
      "0.08316335082054138\n",
      "0.04794181510806084\n",
      "0.05645256116986275\n",
      "0.055097710341215134\n",
      "0.041632503271102905\n",
      "0.013498025946319103\n",
      "0.03134528174996376\n",
      "0.06690237671136856\n",
      "0.07029944658279419\n",
      "0.0370035283267498\n",
      "0.02422330155968666\n",
      "0.1417313516139984\n",
      "0.05704381316900253\n",
      "0.12960253655910492\n",
      "0.12287607043981552\n",
      "0.05381938815116882\n",
      "0.037717778235673904\n",
      "0.03228001669049263\n",
      "0.15333902835845947\n",
      "0.0598645843565464\n",
      "0.12294132262468338\n",
      "0.09055989235639572\n",
      "0.04419853538274765\n",
      "0.07037985324859619\n",
      "0.043372318148612976\n",
      "0.14949394762516022\n",
      "0.06404400616884232\n",
      "0.03764300420880318\n",
      "0.04060574620962143\n",
      "0.03588535264134407\n",
      "0.05069056525826454\n",
      "0.03559857979416847\n",
      "0.06875095516443253\n",
      "0.02314477600157261\n",
      "0.024928294122219086\n",
      "0.12045348435640335\n",
      "0.13124477863311768\n",
      "0.03639674559235573\n",
      "0.06726310402154922\n",
      "0.07269720733165741\n",
      "0.15450042486190796\n",
      "0.06261646747589111\n",
      "0.06154189631342888\n",
      "0.055791184306144714\n",
      "0.02252420037984848\n",
      "0.12311020493507385\n",
      "0.2177523374557495\n",
      "0.07667668163776398\n",
      "0.056343112140893936\n",
      "0.012287553399801254\n",
      "0.05995425581932068\n",
      "0.07734977453947067\n",
      "0.050891924649477005\n",
      "0.04979419335722923\n",
      "0.0412757433950901\n",
      "0.06533483415842056\n",
      "0.15051349997520447\n",
      "0.08946314454078674\n",
      "0.06601807475090027\n",
      "0.07762850821018219\n",
      "0.0568682961165905\n",
      "0.0950278639793396\n",
      "0.06460544466972351\n",
      "0.10480747371912003\n",
      "0.04966405779123306\n",
      "0.02890593558549881\n",
      "0.06599988788366318\n",
      "0.03396555781364441\n",
      "0.02249273471534252\n",
      "0.035322535783052444\n",
      "0.0876709371805191\n",
      "0.008657890371978283\n",
      "0.026641948148608208\n",
      "0.12681782245635986\n",
      "0.05558937415480614\n",
      "0.06300074607133865\n",
      "0.04156196117401123\n",
      "0.022100552916526794\n",
      "0.06839631497859955\n",
      "0.025270290672779083\n",
      "0.04371584579348564\n",
      "0.06205696240067482\n",
      "0.05362558364868164\n",
      "0.12008678168058395\n",
      "0.04924893006682396\n",
      "0.05336302891373634\n",
      "0.05736129730939865\n",
      "0.04025616496801376\n",
      "0.0714724063873291\n",
      "0.047299157828092575\n",
      "0.11718060821294785\n",
      "0.03398563712835312\n",
      "0.01905517466366291\n",
      "0.03944893926382065\n",
      "0.1762515902519226\n",
      "0.07169496268033981\n",
      "Accuracy of the network on the test images: 97.81%\n",
      "Total execution time: 33.78831720352173 seconds\n",
      "Training with lr=0.01, epoch=2\n",
      "Epoch 1/2, Batch 0, Loss: 2.3110454082489014\n",
      "Epoch 1/2, Batch 100, Loss: 0.15055875480175018\n",
      "Epoch 1/2, Batch 200, Loss: 0.06698741763830185\n",
      "Epoch 1/2, Batch 300, Loss: 0.042362164705991745\n",
      "Epoch 1/2, Batch 400, Loss: 0.07791547477245331\n",
      "Epoch 1/2, Batch 500, Loss: 0.12341489642858505\n",
      "Epoch 2/2, Batch 0, Loss: 0.05509233474731445\n",
      "Epoch 2/2, Batch 100, Loss: 0.02438468486070633\n",
      "Epoch 2/2, Batch 200, Loss: 0.0351528637111187\n",
      "Epoch 2/2, Batch 300, Loss: 0.01628848724067211\n",
      "Epoch 2/2, Batch 400, Loss: 0.025816375389695168\n",
      "Epoch 2/2, Batch 500, Loss: 0.007649809587746859\n",
      "Training complete.\n",
      "0.03673568367958069\n",
      "0.012596948072314262\n",
      "0.010357494466006756\n",
      "0.09911984205245972\n",
      "0.04036753624677658\n",
      "0.006903739180415869\n",
      "0.18917852640151978\n",
      "0.025762781500816345\n",
      "0.1264135241508484\n",
      "0.042129036039114\n",
      "0.007651503663510084\n",
      "0.13726559281349182\n",
      "0.014152676798403263\n",
      "0.04127693548798561\n",
      "0.07723604142665863\n",
      "0.05728960409760475\n",
      "0.019500648602843285\n",
      "0.08410655707120895\n",
      "0.03531285747885704\n",
      "0.03965816646814346\n",
      "0.006982084363698959\n",
      "0.10178103297948837\n",
      "0.035398777574300766\n",
      "0.027427369728684425\n",
      "0.04751954227685928\n",
      "0.012243776582181454\n",
      "0.006057211197912693\n",
      "0.02642534300684929\n",
      "0.005184137728065252\n",
      "0.034995030611753464\n",
      "0.013659169897437096\n",
      "0.03527486324310303\n",
      "0.07243307679891586\n",
      "0.07557041943073273\n",
      "0.05834273248910904\n",
      "0.03582208976149559\n",
      "0.03426399827003479\n",
      "0.00987566914409399\n",
      "0.06407388299703598\n",
      "0.0379759706556797\n",
      "0.02562333270907402\n",
      "0.19042037427425385\n",
      "0.01675533503293991\n",
      "0.04324527084827423\n",
      "0.0347488671541214\n",
      "0.04551587253808975\n",
      "0.04613235965371132\n",
      "0.0634080097079277\n",
      "0.06755854934453964\n",
      "0.06166800856590271\n",
      "0.0072417003102600574\n",
      "0.10034217685461044\n",
      "0.00964923482388258\n",
      "0.03348188102245331\n",
      "0.017457883805036545\n",
      "0.010540607385337353\n",
      "0.05599282309412956\n",
      "0.029763393104076385\n",
      "0.07198486477136612\n",
      "0.11066079139709473\n",
      "0.010884192772209644\n",
      "0.034493591636419296\n",
      "0.1141677126288414\n",
      "0.005712479818612337\n",
      "0.0740089863538742\n",
      "0.021749436855316162\n",
      "0.01617184840142727\n",
      "0.0777914747595787\n",
      "0.00796530395746231\n",
      "0.08479592949151993\n",
      "0.02444196678698063\n",
      "0.012678108178079128\n",
      "0.06726904958486557\n",
      "0.03831440582871437\n",
      "0.18900318443775177\n",
      "0.053913746029138565\n",
      "0.032485272735357285\n",
      "0.0692625418305397\n",
      "0.05879136547446251\n",
      "0.004972462542355061\n",
      "0.009074551053345203\n",
      "0.010726327076554298\n",
      "0.018563877791166306\n",
      "0.022906851023435593\n",
      "0.015928832814097404\n",
      "0.01345360279083252\n",
      "0.0033808955922722816\n",
      "0.02591041475534439\n",
      "0.030028950423002243\n",
      "0.018484357744455338\n",
      "0.017384236678481102\n",
      "0.025676406919956207\n",
      "0.05274055525660515\n",
      "0.014296306297183037\n",
      "0.07636084407567978\n",
      "0.019406473264098167\n",
      "0.009855246171355247\n",
      "0.049051713198423386\n",
      "0.03250237554311752\n",
      "0.13669070601463318\n",
      "Accuracy of the network on the test images: 98.53%\n",
      "Total execution time: 60.29885172843933 seconds\n",
      "Training with lr=0.01, epoch=3\n",
      "Epoch 1/3, Batch 0, Loss: 2.307391405105591\n",
      "Epoch 1/3, Batch 100, Loss: 0.06037634238600731\n",
      "Epoch 1/3, Batch 200, Loss: 0.08789301663637161\n",
      "Epoch 1/3, Batch 300, Loss: 0.04608805105090141\n",
      "Epoch 1/3, Batch 400, Loss: 0.0609888881444931\n",
      "Epoch 1/3, Batch 500, Loss: 0.04813750088214874\n",
      "Epoch 2/3, Batch 0, Loss: 0.015252475626766682\n",
      "Epoch 2/3, Batch 100, Loss: 0.050981711596250534\n",
      "Epoch 2/3, Batch 200, Loss: 0.010723886080086231\n",
      "Epoch 2/3, Batch 300, Loss: 0.009394248016178608\n",
      "Epoch 2/3, Batch 400, Loss: 0.016679732128977776\n",
      "Epoch 2/3, Batch 500, Loss: 0.0777115747332573\n",
      "Epoch 3/3, Batch 0, Loss: 0.04928181692957878\n",
      "Epoch 3/3, Batch 100, Loss: 0.01801440306007862\n",
      "Epoch 3/3, Batch 200, Loss: 0.0818478986620903\n",
      "Epoch 3/3, Batch 300, Loss: 0.09961607307195663\n",
      "Epoch 3/3, Batch 400, Loss: 0.022434944286942482\n",
      "Epoch 3/3, Batch 500, Loss: 0.02335578389465809\n",
      "Training complete.\n",
      "0.027633966878056526\n",
      "0.018243985250592232\n",
      "0.004144865088164806\n",
      "0.04243229329586029\n",
      "0.005642999429255724\n",
      "0.03945549577474594\n",
      "0.07216177135705948\n",
      "0.1715262234210968\n",
      "0.03499762713909149\n",
      "0.03720657154917717\n",
      "0.025032250210642815\n",
      "0.03760521858930588\n",
      "0.03763853386044502\n",
      "0.024575673043727875\n",
      "0.012032394297420979\n",
      "0.003243508283048868\n",
      "0.024272657930850983\n",
      "0.013893752358853817\n",
      "0.011201160959899426\n",
      "0.009742800146341324\n",
      "0.027900688350200653\n",
      "0.006001539994031191\n",
      "0.08708608895540237\n",
      "0.030995380133390427\n",
      "0.012948166579008102\n",
      "0.10963492095470428\n",
      "0.009826797991991043\n",
      "0.004926999099552631\n",
      "0.023617001250386238\n",
      "0.06731116771697998\n",
      "0.02519032172858715\n",
      "0.016563721001148224\n",
      "0.0059546250849962234\n",
      "0.001366693526506424\n",
      "0.01728207617998123\n",
      "0.03328217193484306\n",
      "0.013851852156221867\n",
      "0.011623309925198555\n",
      "0.0934956818819046\n",
      "0.07444726675748825\n",
      "0.009419504553079605\n",
      "0.018407514318823814\n",
      "0.15560969710350037\n",
      "0.016457974910736084\n",
      "0.18518517911434174\n",
      "0.03894032537937164\n",
      "0.003799615427851677\n",
      "0.04292811453342438\n",
      "0.06858564168214798\n",
      "0.0805213525891304\n",
      "0.06358462572097778\n",
      "0.0022446156945079565\n",
      "0.057527199387550354\n",
      "0.010035192593932152\n",
      "0.028866324573755264\n",
      "0.04223707318305969\n",
      "0.041385021060705185\n",
      "0.11518452316522598\n",
      "0.003128938376903534\n",
      "0.04127738997340202\n",
      "0.04240531846880913\n",
      "0.05119967460632324\n",
      "0.0451781265437603\n",
      "0.07510792464017868\n",
      "0.02644125185906887\n",
      "0.06903580576181412\n",
      "0.03351688012480736\n",
      "0.165390744805336\n",
      "0.10603270679712296\n",
      "0.028836483135819435\n",
      "0.0871119499206543\n",
      "0.004379801917821169\n",
      "0.11225458234548569\n",
      "0.06351536512374878\n",
      "0.02824261225759983\n",
      "0.08920489996671677\n",
      "0.04890657961368561\n",
      "0.02488587610423565\n",
      "0.01568835787475109\n",
      "0.0055342973209917545\n",
      "0.03386187180876732\n",
      "0.04804849624633789\n",
      "0.02622988261282444\n",
      "0.061511050909757614\n",
      "0.03479726240038872\n",
      "0.03796013444662094\n",
      "0.0030358026269823313\n",
      "0.02802818827331066\n",
      "0.012085109949111938\n",
      "0.02536017633974552\n",
      "0.04929172992706299\n",
      "0.13637705147266388\n",
      "0.038556601852178574\n",
      "0.04241437464952469\n",
      "0.002485428936779499\n",
      "0.03131253272294998\n",
      "0.016390495002269745\n",
      "0.0021800186950713396\n",
      "0.012817157432436943\n",
      "0.019737781956791878\n",
      "Accuracy of the network on the test images: 98.77%\n",
      "Total execution time: 89.07470870018005 seconds\n",
      "Training with lr=0.001, epoch=1\n",
      "Epoch 1/1, Batch 0, Loss: 2.2982709407806396\n",
      "Epoch 1/1, Batch 100, Loss: 0.29406389594078064\n",
      "Epoch 1/1, Batch 200, Loss: 0.13979950547218323\n",
      "Epoch 1/1, Batch 300, Loss: 0.1636635959148407\n",
      "Epoch 1/1, Batch 400, Loss: 0.07083635032176971\n",
      "Epoch 1/1, Batch 500, Loss: 0.09627259522676468\n",
      "Training complete.\n",
      "0.07964891940355301\n",
      "0.12492567300796509\n",
      "0.05310506746172905\n",
      "0.08420885354280472\n",
      "0.05123596265912056\n",
      "0.07395043224096298\n",
      "0.039007652550935745\n",
      "0.10008926689624786\n",
      "0.08974429219961166\n",
      "0.18373674154281616\n",
      "0.0702521875500679\n",
      "0.16452264785766602\n",
      "0.03182019665837288\n",
      "0.06140415742993355\n",
      "0.03813224658370018\n",
      "0.05922168865799904\n",
      "0.07591157406568527\n",
      "0.12853620946407318\n",
      "0.067206472158432\n",
      "0.0481577031314373\n",
      "0.0988425761461258\n",
      "0.04126517474651337\n",
      "0.09287016093730927\n",
      "0.030698316171765327\n",
      "0.06707438826560974\n",
      "0.10106362402439117\n",
      "0.09683690220117569\n",
      "0.0611417181789875\n",
      "0.049653321504592896\n",
      "0.019189005717635155\n",
      "0.034722622483968735\n",
      "0.08851445466279984\n",
      "0.01838335208594799\n",
      "0.03444106876850128\n",
      "0.093426913022995\n",
      "0.10936789214611053\n",
      "0.08700723946094513\n",
      "0.11155782639980316\n",
      "0.08663418143987656\n",
      "0.10308311134576797\n",
      "0.04340992122888565\n",
      "0.029768219217658043\n",
      "0.029128625988960266\n",
      "0.06961092352867126\n",
      "0.03998540714383125\n",
      "0.08446499705314636\n",
      "0.0633748322725296\n",
      "0.09405741095542908\n",
      "0.02757800742983818\n",
      "0.1411421000957489\n",
      "0.13442721962928772\n",
      "0.02121223509311676\n",
      "0.019929001107811928\n",
      "0.03682816028594971\n",
      "0.02497449330985546\n",
      "0.08635742962360382\n",
      "0.0671577900648117\n",
      "0.03898598626255989\n",
      "0.06631200015544891\n",
      "0.14099541306495667\n",
      "0.0744563639163971\n",
      "0.045878201723098755\n",
      "0.040790680795907974\n",
      "0.059377171099185944\n",
      "0.060585979372262955\n",
      "0.08408502489328384\n",
      "0.08450947701931\n",
      "0.05027937889099121\n",
      "0.09361151605844498\n",
      "0.043416839092969894\n",
      "0.07714929431676865\n",
      "0.07736638188362122\n",
      "0.037962041795253754\n",
      "0.05545377358794212\n",
      "0.08278107643127441\n",
      "0.03881348296999931\n",
      "0.14993345737457275\n",
      "0.08861147612333298\n",
      "0.08703804016113281\n",
      "0.09694910049438477\n",
      "0.06944555789232254\n",
      "0.1885271966457367\n",
      "0.09102626144886017\n",
      "0.06982771307229996\n",
      "0.06464432924985886\n",
      "0.14052382111549377\n",
      "0.06651602685451508\n",
      "0.16052688658237457\n",
      "0.04415921866893768\n",
      "0.08074440062046051\n",
      "0.08717445284128189\n",
      "0.14871624112129211\n",
      "0.03558450564742088\n",
      "0.08850531280040741\n",
      "0.06212247908115387\n",
      "0.11562453955411911\n",
      "0.04527508094906807\n",
      "0.01904706284403801\n",
      "0.16939982771873474\n",
      "0.03630544990301132\n",
      "Accuracy of the network on the test images: 97.81%\n",
      "Total execution time: 34.10983729362488 seconds\n",
      "Training with lr=0.001, epoch=2\n",
      "Epoch 1/2, Batch 0, Loss: 2.295623302459717\n",
      "Epoch 1/2, Batch 100, Loss: 0.3386455476284027\n",
      "Epoch 1/2, Batch 200, Loss: 0.1353531777858734\n",
      "Epoch 1/2, Batch 300, Loss: 0.25191640853881836\n",
      "Epoch 1/2, Batch 400, Loss: 0.09354875236749649\n",
      "Epoch 1/2, Batch 500, Loss: 0.0785326361656189\n",
      "Epoch 2/2, Batch 0, Loss: 0.03642278537154198\n",
      "Epoch 2/2, Batch 100, Loss: 0.015145083889365196\n",
      "Epoch 2/2, Batch 200, Loss: 0.040357816964387894\n",
      "Epoch 2/2, Batch 300, Loss: 0.08265076577663422\n",
      "Epoch 2/2, Batch 400, Loss: 0.04762108251452446\n",
      "Epoch 2/2, Batch 500, Loss: 0.04186476767063141\n",
      "Training complete.\n",
      "0.04135630652308464\n",
      "0.03810233622789383\n",
      "0.10112272202968597\n",
      "0.05236661061644554\n",
      "0.05433252826333046\n",
      "0.025005614385008812\n",
      "0.03242131322622299\n",
      "0.0344519279897213\n",
      "0.03951875865459442\n",
      "0.05322197079658508\n",
      "0.024621659889817238\n",
      "0.021788818761706352\n",
      "0.00887306034564972\n",
      "0.010353272780776024\n",
      "0.02627304382622242\n",
      "0.07421061396598816\n",
      "0.017724299803376198\n",
      "0.014675671234726906\n",
      "0.023926233872771263\n",
      "0.029564958065748215\n",
      "0.025843225419521332\n",
      "0.04694841429591179\n",
      "0.08517614006996155\n",
      "0.023424522951245308\n",
      "0.02057943493127823\n",
      "0.032602038234472275\n",
      "0.04073138162493706\n",
      "0.04591045528650284\n",
      "0.007766738533973694\n",
      "0.0648757666349411\n",
      "0.027274105697870255\n",
      "0.04790676757693291\n",
      "0.04838728532195091\n",
      "0.060852471739053726\n",
      "0.06028570979833603\n",
      "0.03706539049744606\n",
      "0.016967013478279114\n",
      "0.07183290272951126\n",
      "0.03436224162578583\n",
      "0.045693784952163696\n",
      "0.02778129279613495\n",
      "0.05324883386492729\n",
      "0.04454424977302551\n",
      "0.020638369023799896\n",
      "0.03005681186914444\n",
      "0.02991575002670288\n",
      "0.011716599576175213\n",
      "0.04267073795199394\n",
      "0.013172483071684837\n",
      "0.02707681432366371\n",
      "0.05259142071008682\n",
      "0.09056984633207321\n",
      "0.04246142879128456\n",
      "0.040605973452329636\n",
      "0.018523767590522766\n",
      "0.0784498006105423\n",
      "0.05364641174674034\n",
      "0.07064855098724365\n",
      "0.09680148214101791\n",
      "0.08759990334510803\n",
      "0.028828715905547142\n",
      "0.05177683383226395\n",
      "0.02168150246143341\n",
      "0.013940112665295601\n",
      "0.0722588300704956\n",
      "0.054312873631715775\n",
      "0.013242017477750778\n",
      "0.05995912104845047\n",
      "0.011662624776363373\n",
      "0.014393056742846966\n",
      "0.03163274750113487\n",
      "0.007889529690146446\n",
      "0.04633823037147522\n",
      "0.0773744285106659\n",
      "0.05622342973947525\n",
      "0.021364405751228333\n",
      "0.0243176631629467\n",
      "0.012892339378595352\n",
      "0.015890203416347504\n",
      "0.03488899767398834\n",
      "0.020252957940101624\n",
      "0.015364503487944603\n",
      "0.007038872689008713\n",
      "0.06470158696174622\n",
      "0.021670851856470108\n",
      "0.053359851241111755\n",
      "0.01989973522722721\n",
      "0.11977347731590271\n",
      "0.03832659497857094\n",
      "0.09683100879192352\n",
      "0.04740779474377632\n",
      "0.019190121442079544\n",
      "0.042266204953193665\n",
      "0.0657356008887291\n",
      "0.09925046563148499\n",
      "0.08683542162179947\n",
      "0.10048606246709824\n",
      "0.01535782590508461\n",
      "0.019377486780285835\n",
      "0.043087344616651535\n",
      "Accuracy of the network on the test images: 98.67%\n",
      "Total execution time: 61.88913583755493 seconds\n",
      "Training with lr=0.001, epoch=3\n",
      "Epoch 1/3, Batch 0, Loss: 2.305610418319702\n",
      "Epoch 1/3, Batch 100, Loss: 0.2588386833667755\n",
      "Epoch 1/3, Batch 200, Loss: 0.31960877776145935\n",
      "Epoch 1/3, Batch 300, Loss: 0.13371038436889648\n",
      "Epoch 1/3, Batch 400, Loss: 0.08148861676454544\n",
      "Epoch 1/3, Batch 500, Loss: 0.12161194533109665\n",
      "Epoch 2/3, Batch 0, Loss: 0.08608430624008179\n",
      "Epoch 2/3, Batch 100, Loss: 0.02044055238366127\n",
      "Epoch 2/3, Batch 200, Loss: 0.046570420265197754\n",
      "Epoch 2/3, Batch 300, Loss: 0.05498116463422775\n",
      "Epoch 2/3, Batch 400, Loss: 0.016722343862056732\n",
      "Epoch 2/3, Batch 500, Loss: 0.05000805854797363\n",
      "Epoch 3/3, Batch 0, Loss: 0.01758236438035965\n",
      "Epoch 3/3, Batch 100, Loss: 0.10725916177034378\n",
      "Epoch 3/3, Batch 200, Loss: 0.026568980887532234\n",
      "Epoch 3/3, Batch 300, Loss: 0.03814861178398132\n",
      "Epoch 3/3, Batch 400, Loss: 0.08774691820144653\n",
      "Epoch 3/3, Batch 500, Loss: 0.034030988812446594\n",
      "Training complete.\n",
      "0.06394222378730774\n",
      "0.12197818607091904\n",
      "0.07910602539777756\n",
      "0.12124743312597275\n",
      "0.023193374276161194\n",
      "0.02607155591249466\n",
      "0.024703137576580048\n",
      "0.02322220988571644\n",
      "0.015566353686153889\n",
      "0.007162357680499554\n",
      "0.016399085521697998\n",
      "0.04473567008972168\n",
      "0.04251580312848091\n",
      "0.0827777311205864\n",
      "0.06616665422916412\n",
      "0.014061277732253075\n",
      "0.16467244923114777\n",
      "0.09272530674934387\n",
      "0.02989349514245987\n",
      "0.059630442410707474\n",
      "0.03718804940581322\n",
      "0.07949155569076538\n",
      "0.012224040925502777\n",
      "0.04214595630764961\n",
      "0.06223515421152115\n",
      "0.015752896666526794\n",
      "0.00946839526295662\n",
      "0.06084221228957176\n",
      "0.003130305325612426\n",
      "0.08075820654630661\n",
      "0.049236513674259186\n",
      "0.0202182549983263\n",
      "0.0031623723916709423\n",
      "0.013154618442058563\n",
      "0.08941778540611267\n",
      "0.010058829560875893\n",
      "0.05265868827700615\n",
      "0.0107875382527709\n",
      "0.0093813082203269\n",
      "0.03614058718085289\n",
      "0.08994723111391068\n",
      "0.026444299146533012\n",
      "0.004232778679579496\n",
      "0.0375773161649704\n",
      "0.01245213020592928\n",
      "0.01967509463429451\n",
      "0.014467792585492134\n",
      "0.1289113461971283\n",
      "0.017572328448295593\n",
      "0.04719505459070206\n",
      "0.08088010549545288\n",
      "0.004538706969469786\n",
      "0.026297813281416893\n",
      "0.05644512176513672\n",
      "0.0026525333523750305\n",
      "0.06106547266244888\n",
      "0.008546259254217148\n",
      "0.049761053174734116\n",
      "0.019417965784668922\n",
      "0.050609689205884933\n",
      "0.015343126840889454\n",
      "0.018184876069426537\n",
      "0.06828721612691879\n",
      "0.03333679586648941\n",
      "0.04775869846343994\n",
      "0.01988111063838005\n",
      "0.07035437226295471\n",
      "0.029205085709691048\n",
      "0.04478519782423973\n",
      "0.07181025296449661\n",
      "0.023430339992046356\n",
      "0.056603528559207916\n",
      "0.08599266409873962\n",
      "0.019762177020311356\n",
      "0.02387327142059803\n",
      "0.0378781259059906\n",
      "0.017744150012731552\n",
      "0.040207624435424805\n",
      "0.039921969175338745\n",
      "0.07650485634803772\n",
      "0.03236278146505356\n",
      "0.051741134375333786\n",
      "0.019182877615094185\n",
      "0.008227206766605377\n",
      "0.013824370689690113\n",
      "0.05650119483470917\n",
      "0.022012479603290558\n",
      "0.02669270522892475\n",
      "0.08542676270008087\n",
      "0.0688149705529213\n",
      "0.00894028227776289\n",
      "0.03613556921482086\n",
      "0.09900882095098495\n",
      "0.027769550681114197\n",
      "0.016563553363084793\n",
      "0.04223835840821266\n",
      "0.035441942512989044\n",
      "0.030344348400831223\n",
      "0.04350050538778305\n",
      "0.040317319333553314\n",
      "Accuracy of the network on the test images: 98.68%\n",
      "Total execution time: 89.29948043823242 seconds\n",
      "Training with lr=0.0005, epoch=1\n",
      "Epoch 1/1, Batch 0, Loss: 2.3034746646881104\n",
      "Epoch 1/1, Batch 100, Loss: 0.2960226237773895\n",
      "Epoch 1/1, Batch 200, Loss: 0.24835985898971558\n",
      "Epoch 1/1, Batch 300, Loss: 0.15207846462726593\n",
      "Epoch 1/1, Batch 400, Loss: 0.08120433986186981\n",
      "Epoch 1/1, Batch 500, Loss: 0.060414209961891174\n",
      "Training complete.\n",
      "0.07084766030311584\n",
      "0.09409786015748978\n",
      "0.08533968031406403\n",
      "0.056467510759830475\n",
      "0.17538422346115112\n",
      "0.07847519218921661\n",
      "0.05545082688331604\n",
      "0.0752469077706337\n",
      "0.12226302921772003\n",
      "0.11817152053117752\n",
      "0.12935517728328705\n",
      "0.13751783967018127\n",
      "0.16876745223999023\n",
      "0.06228449195623398\n",
      "0.13728241622447968\n",
      "0.1481364667415619\n",
      "0.07378176599740982\n",
      "0.08776875585317612\n",
      "0.03257441893219948\n",
      "0.04825478419661522\n",
      "0.0905633345246315\n",
      "0.10221254080533981\n",
      "0.06557940691709518\n",
      "0.06655475497245789\n",
      "0.1345694661140442\n",
      "0.22717447578907013\n",
      "0.03561963513493538\n",
      "0.0863959789276123\n",
      "0.13237209618091583\n",
      "0.0841694101691246\n",
      "0.08065465837717056\n",
      "0.09506600350141525\n",
      "0.1438458114862442\n",
      "0.042279310524463654\n",
      "0.048224709928035736\n",
      "0.09372670948505402\n",
      "0.04047756642103195\n",
      "0.1629326045513153\n",
      "0.04215283319354057\n",
      "0.12757612764835358\n",
      "0.039211735129356384\n",
      "0.10595124959945679\n",
      "0.02951669692993164\n",
      "0.05269787460565567\n",
      "0.10216619819402695\n",
      "0.1019856333732605\n",
      "0.06533677875995636\n",
      "0.07470174133777618\n",
      "0.03946416452527046\n",
      "0.11644244939088821\n",
      "0.06100012734532356\n",
      "0.07341676205396652\n",
      "0.13099877536296844\n",
      "0.14235807955265045\n",
      "0.23956798017024994\n",
      "0.07492192089557648\n",
      "0.0989319309592247\n",
      "0.09273257106542587\n",
      "0.11824838817119598\n",
      "0.06561114639043808\n",
      "0.10620815306901932\n",
      "0.03251428157091141\n",
      "0.02692718245089054\n",
      "0.12869812548160553\n",
      "0.14412465691566467\n",
      "0.17049700021743774\n",
      "0.039597734808921814\n",
      "0.15169164538383484\n",
      "0.1656784564256668\n",
      "0.09184738993644714\n",
      "0.15569354593753815\n",
      "0.1317521333694458\n",
      "0.03743070363998413\n",
      "0.09376263618469238\n",
      "0.0886775553226471\n",
      "0.09620606154203415\n",
      "0.1284419298171997\n",
      "0.06135931983590126\n",
      "0.1388537436723709\n",
      "0.0961531549692154\n",
      "0.09027650952339172\n",
      "0.11918681859970093\n",
      "0.1285998672246933\n",
      "0.1373993158340454\n",
      "0.14262284338474274\n",
      "0.0739603117108345\n",
      "0.11691784858703613\n",
      "0.08109640330076218\n",
      "0.18082886934280396\n",
      "0.053864091634750366\n",
      "0.12986761331558228\n",
      "0.07351180911064148\n",
      "0.05805555731058121\n",
      "0.07449103891849518\n",
      "0.04959700256586075\n",
      "0.04758452996611595\n",
      "0.11791548132896423\n",
      "0.10527078807353973\n",
      "0.07731190323829651\n",
      "0.10339049249887466\n",
      "Accuracy of the network on the test images: 97.01%\n",
      "Total execution time: 33.951717376708984 seconds\n",
      "Training with lr=0.0005, epoch=2\n",
      "Epoch 1/2, Batch 0, Loss: 2.2992162704467773\n",
      "Epoch 1/2, Batch 100, Loss: 0.3702523112297058\n",
      "Epoch 1/2, Batch 200, Loss: 0.20947803556919098\n",
      "Epoch 1/2, Batch 300, Loss: 0.2493744045495987\n",
      "Epoch 1/2, Batch 400, Loss: 0.1871197521686554\n",
      "Epoch 1/2, Batch 500, Loss: 0.09564076364040375\n",
      "Epoch 2/2, Batch 0, Loss: 0.17586317658424377\n",
      "Epoch 2/2, Batch 100, Loss: 0.22201573848724365\n",
      "Epoch 2/2, Batch 200, Loss: 0.07490665465593338\n",
      "Epoch 2/2, Batch 300, Loss: 0.0753554180264473\n",
      "Epoch 2/2, Batch 400, Loss: 0.04553767666220665\n",
      "Epoch 2/2, Batch 500, Loss: 0.11037513613700867\n",
      "Training complete.\n",
      "0.029469121247529984\n",
      "0.048890773206949234\n",
      "0.11966188251972198\n",
      "0.05122930184006691\n",
      "0.024211356416344643\n",
      "0.05369609221816063\n",
      "0.04519740119576454\n",
      "0.01990361139178276\n",
      "0.10290059447288513\n",
      "0.041929300874471664\n",
      "0.06558536738157272\n",
      "0.028991971164941788\n",
      "0.09735897183418274\n",
      "0.046156588941812515\n",
      "0.021761726588010788\n",
      "0.028326366096735\n",
      "0.0497053824365139\n",
      "0.04526721313595772\n",
      "0.1077800840139389\n",
      "0.08289605379104614\n",
      "0.08693569898605347\n",
      "0.044112369418144226\n",
      "0.10193677991628647\n",
      "0.025122875347733498\n",
      "0.10341770946979523\n",
      "0.08069686591625214\n",
      "0.15568101406097412\n",
      "0.02065705694258213\n",
      "0.007867676205933094\n",
      "0.09102106839418411\n",
      "0.052014488726854324\n",
      "0.0670723021030426\n",
      "0.034197721630334854\n",
      "0.11698018014431\n",
      "0.0395745187997818\n",
      "0.04781670868396759\n",
      "0.0698685422539711\n",
      "0.03983624279499054\n",
      "0.010279620997607708\n",
      "0.14163759350776672\n",
      "0.038903214037418365\n",
      "0.03689130023121834\n",
      "0.07972947508096695\n",
      "0.12142003327608109\n",
      "0.05629134550690651\n",
      "0.03325991332530975\n",
      "0.02202109806239605\n",
      "0.14621643722057343\n",
      "0.03747524321079254\n",
      "0.04442504048347473\n",
      "0.07428892701864243\n",
      "0.026924846693873405\n",
      "0.08646617084741592\n",
      "0.060306910425424576\n",
      "0.05050650238990784\n",
      "0.018323654308915138\n",
      "0.16076716780662537\n",
      "0.05327010527253151\n",
      "0.01773116923868656\n",
      "0.013514025136828423\n",
      "0.08740022033452988\n",
      "0.04273701831698418\n",
      "0.07794991880655289\n",
      "0.053605977445840836\n",
      "0.075590580701828\n",
      "0.014445487409830093\n",
      "0.03463801369071007\n",
      "0.07359042018651962\n",
      "0.06655848771333694\n",
      "0.09642614424228668\n",
      "0.10352438688278198\n",
      "0.0469488650560379\n",
      "0.13773055374622345\n",
      "0.04395528882741928\n",
      "0.028065631166100502\n",
      "0.02440432831645012\n",
      "0.14633570611476898\n",
      "0.02274039015173912\n",
      "0.09492761641740799\n",
      "0.03435036167502403\n",
      "0.07224124670028687\n",
      "0.05272549018263817\n",
      "0.03751793131232262\n",
      "0.05365047603845596\n",
      "0.04433624818921089\n",
      "0.01041214819997549\n",
      "0.05450437590479851\n",
      "0.04411639645695686\n",
      "0.08052850514650345\n",
      "0.08679188787937164\n",
      "0.11135587096214294\n",
      "0.05054602026939392\n",
      "0.0475129559636116\n",
      "0.0943671390414238\n",
      "0.07333419471979141\n",
      "0.013661598786711693\n",
      "0.04869571328163147\n",
      "0.025423934683203697\n",
      "0.13292309641838074\n",
      "0.04539071023464203\n",
      "Accuracy of the network on the test images: 97.95%\n",
      "Total execution time: 59.78125834465027 seconds\n",
      "Training with lr=0.0005, epoch=3\n",
      "Epoch 1/3, Batch 0, Loss: 2.3059637546539307\n",
      "Epoch 1/3, Batch 100, Loss: 0.44896048307418823\n",
      "Epoch 1/3, Batch 200, Loss: 0.3055581748485565\n",
      "Epoch 1/3, Batch 300, Loss: 0.2056991457939148\n",
      "Epoch 1/3, Batch 400, Loss: 0.21166376769542694\n",
      "Epoch 1/3, Batch 500, Loss: 0.084098681807518\n",
      "Epoch 2/3, Batch 0, Loss: 0.1764202117919922\n",
      "Epoch 2/3, Batch 100, Loss: 0.045123592019081116\n",
      "Epoch 2/3, Batch 200, Loss: 0.14558610320091248\n",
      "Epoch 2/3, Batch 300, Loss: 0.08574819564819336\n",
      "Epoch 2/3, Batch 400, Loss: 0.14186887443065643\n",
      "Epoch 2/3, Batch 500, Loss: 0.10324668139219284\n",
      "Epoch 3/3, Batch 0, Loss: 0.05397210642695427\n",
      "Epoch 3/3, Batch 100, Loss: 0.07350313663482666\n",
      "Epoch 3/3, Batch 200, Loss: 0.08357299864292145\n",
      "Epoch 3/3, Batch 300, Loss: 0.1459207683801651\n",
      "Epoch 3/3, Batch 400, Loss: 0.09419748932123184\n",
      "Epoch 3/3, Batch 500, Loss: 0.03102482296526432\n",
      "Training complete.\n",
      "0.01304876059293747\n",
      "0.015659363940358162\n",
      "0.008159214630723\n",
      "0.18057940900325775\n",
      "0.05932449921965599\n",
      "0.057501472532749176\n",
      "0.036152683198451996\n",
      "0.019405042752623558\n",
      "0.10710379481315613\n",
      "0.025834714993834496\n",
      "0.08418399095535278\n",
      "0.013492199592292309\n",
      "0.053793326020240784\n",
      "0.04001358896493912\n",
      "0.061951894313097\n",
      "0.032826926559209824\n",
      "0.0342903770506382\n",
      "0.03376835957169533\n",
      "0.046734895557165146\n",
      "0.051050420850515366\n",
      "0.014936582185328007\n",
      "0.01953105255961418\n",
      "0.05299518257379532\n",
      "0.022523153573274612\n",
      "0.06969261914491653\n",
      "0.0491487979888916\n",
      "0.0329415425658226\n",
      "0.08356416970491409\n",
      "0.09022011607885361\n",
      "0.022933732718229294\n",
      "0.06370005011558533\n",
      "0.0252991896122694\n",
      "0.045329075306653976\n",
      "0.028222646564245224\n",
      "0.07073495537042618\n",
      "0.07752100378274918\n",
      "0.07971760630607605\n",
      "0.03384595736861229\n",
      "0.056371401995420456\n",
      "0.02093607373535633\n",
      "0.08182356506586075\n",
      "0.01022351998835802\n",
      "0.03084767609834671\n",
      "0.04859394580125809\n",
      "0.0512096993625164\n",
      "0.013183390721678734\n",
      "0.013967215083539486\n",
      "0.04117853194475174\n",
      "0.00443975580856204\n",
      "0.03314637020230293\n",
      "0.023584485054016113\n",
      "0.1333320587873459\n",
      "0.015119330957531929\n",
      "0.07236354053020477\n",
      "0.019838552922010422\n",
      "0.0663200318813324\n",
      "0.06574198603630066\n",
      "0.08515597134828568\n",
      "0.06903442740440369\n",
      "0.049083031713962555\n",
      "0.06794293224811554\n",
      "0.012743770144879818\n",
      "0.012718497775495052\n",
      "0.02964896894991398\n",
      "0.0672573447227478\n",
      "0.016645871102809906\n",
      "0.03439837694168091\n",
      "0.046909354627132416\n",
      "0.02493264153599739\n",
      "0.07178839296102524\n",
      "0.04740424454212189\n",
      "0.03527291864156723\n",
      "0.03746727108955383\n",
      "0.062058426439762115\n",
      "0.008506632409989834\n",
      "0.0731051042675972\n",
      "0.020645378157496452\n",
      "0.07707243412733078\n",
      "0.04582112655043602\n",
      "0.02233724854886532\n",
      "0.05794186145067215\n",
      "0.05432453006505966\n",
      "0.012136362493038177\n",
      "0.0504847951233387\n",
      "0.03746402636170387\n",
      "0.03572344034910202\n",
      "0.04344681650400162\n",
      "0.025713704526424408\n",
      "0.06633706390857697\n",
      "0.02047646977007389\n",
      "0.0632319375872612\n",
      "0.062329765409231186\n",
      "0.07149134576320648\n",
      "0.04566728696227074\n",
      "0.07901211082935333\n",
      "0.008151937276124954\n",
      "0.02800314873456955\n",
      "0.07490453124046326\n",
      "0.02460920624434948\n",
      "0.026641778647899628\n",
      "Accuracy of the network on the test images: 98.52%\n",
      "Total execution time: 87.28176259994507 seconds\n"
     ]
    }
   ],
   "source": [
    "results = {}\n",
    "\n",
    "for lr, epoch_n in param_combinations:\n",
    "\n",
    "    # Start program timer\n",
    "    start_time = time.time()\n",
    "\n",
    "    current_hyperparameters = str(lr)+\",\"+str(epoch_n)\n",
    "\n",
    "    results[current_hyperparameters] = {\n",
    "        'train_losses': [],\n",
    "        'val_losses': [],\n",
    "        'accuracies': [],\n",
    "        'execution_time': []\n",
    "    }\n",
    "\n",
    "    print(f\"Training with lr={lr}, epoch={epoch_n}\")\n",
    "\n",
    "    \n",
    "    # Instantiate the network, loss function and optimizer\n",
    "    net = CNN().to(compute_device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    opt = optim.Adam(net.parameters(), lr=lr)\n",
    "    \n",
    "    # Training Procedure\n",
    "    def train_model(epochs, network, loaders):\n",
    "        network.train()  # Set the network to training mode\n",
    "    \n",
    "        for e in range(epochs):\n",
    "            for batch_idx, (inputs, targets) in enumerate(loaders['train_loader']):\n",
    "                inputs, targets = inputs.to(compute_device), targets.to(compute_device)\n",
    "                network.zero_grad()\n",
    "                outputs = network.forward_pass(inputs)\n",
    "                loss = criterion(outputs, targets)\n",
    "                loss.backward()\n",
    "                opt.step()\n",
    "\n",
    "                results[current_hyperparameters]['train_losses'].append(loss.item())\n",
    "    \n",
    "                if batch_idx % 100 == 0:\n",
    "                    print(f'Epoch {e+1}/{epochs}, Batch {batch_idx}, Loss: {loss.item()}')\n",
    "    \n",
    "        print(\"Training complete.\")\n",
    "    \n",
    "    train_model(epoch_n, net, data_loaders)\n",
    "    \n",
    "    # Function to evaluate the model performance on the test dataset\n",
    "    def evaluate_model(network, loaders):\n",
    "        network.eval()  # Set the network to evaluation mode\n",
    "        correct = 0\n",
    "        total = 0\n",
    "    \n",
    "        with torch.no_grad():  # No need to track gradients for validation\n",
    "            for inputs, targets in loaders['test_loader']:\n",
    "                inputs, targets = inputs.to(compute_device), targets.to(compute_device)\n",
    "                outputs = network.forward_pass(inputs)\n",
    "\n",
    "                val_loss = criterion(outputs, targets).item()\n",
    "                results[current_hyperparameters]['val_losses'].append(val_loss)\n",
    "                \n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += targets.size(0)\n",
    "                correct += (predicted == targets).sum().item()\n",
    "    \n",
    "        accuracy = 100 * correct / total\n",
    "        results[current_hyperparameters]['accuracies'].append(accuracy)\n",
    "        print(f'Accuracy of the network on the test images: {accuracy:.2f}%')\n",
    "    \n",
    "    # Evaluate the trained model\n",
    "    evaluate_model(net, data_loaders)\n",
    "    \n",
    "    # End program timer\n",
    "    end_time = time.time()\n",
    "    total_time = end_time - start_time\n",
    "    results[current_hyperparameters]['execution_time'].append(total_time)\n",
    "    print(f\"Total execution time: {total_time} seconds\")\n",
    "\n",
    "# Save dictionary to a JSON file\n",
    "with open('CNNresults.json', 'w') as json_file:\n",
    "    json.dump(results, json_file, indent=4)  # `indent` makes the file human-readable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b162cd6-7064-4fbc-8193-0fd809f51ed8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
